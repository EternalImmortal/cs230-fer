{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gwdg7Sv3XBaP",
    "outputId": "f573015e-64ef-465d-b2ee-2c94b4046cfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nz38mJZXN_P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "BS = 256\n",
    "DROPOUT_RATE = 0.5\n",
    "SGD_LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#data = pd.read_csv('fer2013/icml_face_data.csv', sep=r'\\s,\\s', header=0, encoding='ascii', engine='python') \n",
    "data = pd.read_csv('fer2013/icml_face_data.csv') \n",
    "data.head()\n",
    "\n",
    "data.columns\n",
    "\n",
    "data_train = data[data[' Usage'] == 'Training']\n",
    "#print('Number samples in the training dataset: ', data_train.shape[0])\n",
    "\n",
    "data_dev = data[data[' Usage'] == 'PublicTest']\n",
    "#print('Number samples in the development dataset: ', data_dev.shape[0]) print(data_dev.head())\n",
    "\n",
    "data_test = data[data[' Usage'] == 'PrivateTest']\n",
    "#print('Number samples in the development dataset: ', data_dev.shape[0]) print(data_test.head())\n",
    "\n",
    "data_train.to_csv('fer2013/train.csv') \n",
    "data_dev.to_csv('fer2013/dev.csv') \n",
    "data_test.to_csv('fer2013/test.csv')\n",
    "\n",
    "print(data_train.shape) \n",
    "print(data_dev.shape) \n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAjh6yOLYPZm"
   },
   "outputs": [],
   "source": [
    "# Function that reads the data from the csv file, increases the size of the images and returns the images and their labels\n",
    "    # dataset: Data path\n",
    "def get_data(dataset):\n",
    "    \n",
    "    file_stream = file_io.FileIO(dataset, mode='r')\n",
    "    data = pd.read_csv(file_stream)\n",
    "\n",
    "    #data = pd.read_csv('fer2013/fer2013.csv')\n",
    "    data[' pixels'] = data[' pixels'].apply(lambda x: [int(pixel) for pixel in x.split()])\n",
    "\n",
    "    # Retrieve train input and target\n",
    "    X, Y = data[' pixels'].tolist(), data['emotion'].values\n",
    "    \n",
    "    # Reshape images to 4D (num_samples, width, height, num_channels)\n",
    "    X_res = np.array(X, dtype='float32').reshape(-1,48,48,1)\n",
    "    # Normalize images with max (the maximum pixel intensity is 255)\n",
    "    X_res = X_res/255.0\n",
    "    #image_resized = resize(image, (image.shape[0] // 4, image.shape[1] // 4), anti_aliasing=True)\n",
    "\n",
    "    Y_res = np.zeros((Y.size, 7))\n",
    "    Y_res[np.arange(Y.size),Y] = 1    \n",
    "    \n",
    "    return  X_res, Y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_dir = 'fer2013/train.csv'\n",
    "dev_dataset_dir = 'fer2013/dev.csv'\n",
    "# Data preparation\n",
    "X_train, Y_train  = get_data(training_dataset_dir)\n",
    "X_dev, Y_dev      = get_data(dev_dataset_dir)\n",
    "\n",
    "# Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely\n",
    "# rescale:          Rescaling factor (defaults to None). Multiply the data by the value provided (before applying any other transformation)\n",
    "# rotation_range:   Int. Degree range for random rotations\n",
    "# shear_range:      Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "# zoom_range:       Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]\n",
    "# fill_mode :       Points outside the boundaries of the input are filled according to the given mode: {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}\n",
    "# horizontal_flip:  Boolean. Randomly flip inputs horizontally\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range  = 10,\n",
    "    shear_range     = 5, # 10 degrees\n",
    "    zoom_range      = 0.1,\n",
    "    fill_mode       = 'reflect',\n",
    "    horizontal_flip = True)\n",
    "\n",
    "# Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batcfillhes indefinitely, in an infinite loop\n",
    "    # x:            Data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, \n",
    "    #               it should have value 3\n",
    "    # y:            Labels\n",
    "    # batch_size:   Int (default: 32)\n",
    "train_generator = train_datagen.flow(X_train, Y_train,  batch_size  = BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement below paper CPCPCPFF depth 5, 2.4m params\n",
    "# http://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Kim_Fusing_Aligned_and_CVPR_2016_paper.pdf\n",
    "# Reference: https://arxiv.org/pdf/1612.02903.pdf\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5, 5), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(96, (5, 5), activation='relu',padding='same',name=\"conv2\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(256, (5, 5), activation='relu',padding='same',name=\"conv3\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
    "model.add(Conv2D(256, (5, 5), activation='relu',padding='same',name=\"conv4\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation='relu',name='fc1'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
    "\n",
    "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
    "sgd = SGD(lr=0.01,momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "112/112 [==============================] - 31s 273ms/step - loss: 1.8229 - acc: 0.2475 - val_loss: 1.8408 - val_acc: 0.2494\n",
      "Epoch 2/300\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 1.7963 - acc: 0.2529 - val_loss: 1.8246 - val_acc: 0.2435\n",
      "Epoch 3/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.7681 - acc: 0.2680 - val_loss: 1.7777 - val_acc: 0.2747\n",
      "Epoch 4/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.7386 - acc: 0.2955 - val_loss: 1.7289 - val_acc: 0.3188\n",
      "Epoch 5/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.7106 - acc: 0.3151 - val_loss: 1.7116 - val_acc: 0.3101\n",
      "Epoch 6/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.6803 - acc: 0.3347 - val_loss: 1.6934 - val_acc: 0.3238\n",
      "Epoch 7/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.6452 - acc: 0.3560 - val_loss: 1.6455 - val_acc: 0.3617\n",
      "Epoch 8/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.6162 - acc: 0.3718 - val_loss: 1.5870 - val_acc: 0.3865\n",
      "Epoch 9/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.5847 - acc: 0.3857 - val_loss: 1.5671 - val_acc: 0.3901\n",
      "Epoch 10/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.5562 - acc: 0.3977 - val_loss: 1.5070 - val_acc: 0.4188\n",
      "Epoch 11/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.5263 - acc: 0.4103 - val_loss: 1.4932 - val_acc: 0.4322\n",
      "Epoch 12/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.5102 - acc: 0.4178 - val_loss: 1.4481 - val_acc: 0.4444\n",
      "Epoch 13/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.4838 - acc: 0.4265 - val_loss: 1.4381 - val_acc: 0.4486\n",
      "Epoch 14/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.4668 - acc: 0.4369 - val_loss: 1.4172 - val_acc: 0.4583\n",
      "Epoch 15/300\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 1.4473 - acc: 0.4404 - val_loss: 1.3789 - val_acc: 0.4753\n",
      "Epoch 16/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.4291 - acc: 0.4506 - val_loss: 1.3522 - val_acc: 0.4837\n",
      "Epoch 17/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 1.4173 - acc: 0.4581 - val_loss: 1.3535 - val_acc: 0.4792\n",
      "Epoch 18/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 1.3863 - acc: 0.4683 - val_loss: 1.3339 - val_acc: 0.4851\n",
      "Epoch 19/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 1.3783 - acc: 0.4716 - val_loss: 1.3196 - val_acc: 0.4929\n",
      "Epoch 20/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 1.3598 - acc: 0.4751 - val_loss: 1.2956 - val_acc: 0.5021\n",
      "Epoch 21/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 1.3463 - acc: 0.4830 - val_loss: 1.2896 - val_acc: 0.5068\n",
      "Epoch 22/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 1.3243 - acc: 0.4928 - val_loss: 1.2657 - val_acc: 0.5171\n",
      "Epoch 23/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.3111 - acc: 0.5007 - val_loss: 1.2573 - val_acc: 0.5213\n",
      "Epoch 24/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 1.2887 - acc: 0.5074 - val_loss: 1.2391 - val_acc: 0.5266\n",
      "Epoch 25/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.2910 - acc: 0.5066 - val_loss: 1.2351 - val_acc: 0.5235\n",
      "Epoch 26/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.2696 - acc: 0.5175 - val_loss: 1.2103 - val_acc: 0.5419\n",
      "Epoch 27/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.2574 - acc: 0.5194 - val_loss: 1.2102 - val_acc: 0.5403\n",
      "Epoch 28/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.2426 - acc: 0.5251 - val_loss: 1.2040 - val_acc: 0.5453\n",
      "Epoch 29/300\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 1.2282 - acc: 0.5330 - val_loss: 1.1967 - val_acc: 0.5464\n",
      "Epoch 30/300\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 1.2083 - acc: 0.5419 - val_loss: 1.1783 - val_acc: 0.5570\n",
      "Epoch 31/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.2061 - acc: 0.5411 - val_loss: 1.1948 - val_acc: 0.5425\n",
      "Epoch 32/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.1901 - acc: 0.5500 - val_loss: 1.1597 - val_acc: 0.5614\n",
      "Epoch 33/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.1748 - acc: 0.5550 - val_loss: 1.1543 - val_acc: 0.5715\n",
      "Epoch 34/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.1689 - acc: 0.5567 - val_loss: 1.1480 - val_acc: 0.5687\n",
      "Epoch 35/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 1.1481 - acc: 0.5680 - val_loss: 1.1524 - val_acc: 0.5617\n",
      "Epoch 36/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.1422 - acc: 0.5697 - val_loss: 1.1374 - val_acc: 0.5748\n",
      "Epoch 37/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.1280 - acc: 0.5745 - val_loss: 1.1347 - val_acc: 0.5784\n",
      "Epoch 38/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.1090 - acc: 0.5830 - val_loss: 1.1379 - val_acc: 0.5690\n",
      "Epoch 39/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.0962 - acc: 0.5874 - val_loss: 1.1585 - val_acc: 0.5642\n",
      "Epoch 40/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.0987 - acc: 0.5873 - val_loss: 1.1231 - val_acc: 0.5871\n",
      "Epoch 41/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.0898 - acc: 0.5898 - val_loss: 1.1243 - val_acc: 0.5807\n",
      "Epoch 42/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.0654 - acc: 0.5976 - val_loss: 1.1162 - val_acc: 0.5837\n",
      "Epoch 43/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.0576 - acc: 0.6032 - val_loss: 1.1068 - val_acc: 0.5952\n",
      "Epoch 44/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.0454 - acc: 0.6041 - val_loss: 1.1056 - val_acc: 0.5954\n",
      "Epoch 45/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.0385 - acc: 0.6095 - val_loss: 1.1076 - val_acc: 0.5901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 1.0238 - acc: 0.6123 - val_loss: 1.1054 - val_acc: 0.5913\n",
      "Epoch 47/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.0127 - acc: 0.6227 - val_loss: 1.1048 - val_acc: 0.5921\n",
      "Epoch 48/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 1.0100 - acc: 0.6213 - val_loss: 1.0936 - val_acc: 0.5982\n",
      "Epoch 49/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.9836 - acc: 0.6351 - val_loss: 1.0917 - val_acc: 0.5991\n",
      "Epoch 50/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.9730 - acc: 0.6383 - val_loss: 1.1172 - val_acc: 0.6004\n",
      "Epoch 51/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.9600 - acc: 0.6394 - val_loss: 1.0959 - val_acc: 0.6060\n",
      "Epoch 52/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.9474 - acc: 0.6460 - val_loss: 1.1138 - val_acc: 0.5974\n",
      "Epoch 53/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.9456 - acc: 0.6486 - val_loss: 1.0923 - val_acc: 0.6066\n",
      "Epoch 54/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.9333 - acc: 0.6539 - val_loss: 1.0859 - val_acc: 0.6035\n",
      "Epoch 55/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.9123 - acc: 0.6616 - val_loss: 1.1020 - val_acc: 0.6038\n",
      "Epoch 56/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.8995 - acc: 0.6631 - val_loss: 1.1015 - val_acc: 0.6046\n",
      "Epoch 57/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.8869 - acc: 0.6671 - val_loss: 1.0999 - val_acc: 0.5968\n",
      "Epoch 58/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.8699 - acc: 0.6767 - val_loss: 1.0978 - val_acc: 0.6069\n",
      "Epoch 59/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.8631 - acc: 0.6800 - val_loss: 1.1015 - val_acc: 0.6074\n",
      "Epoch 60/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.8441 - acc: 0.6863 - val_loss: 1.1095 - val_acc: 0.6088\n",
      "Epoch 61/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.8339 - acc: 0.6921 - val_loss: 1.1124 - val_acc: 0.6096\n",
      "Epoch 62/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.8270 - acc: 0.6940 - val_loss: 1.1053 - val_acc: 0.6082\n",
      "Epoch 63/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.8043 - acc: 0.7008 - val_loss: 1.1152 - val_acc: 0.6135\n",
      "Epoch 64/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.7960 - acc: 0.7037 - val_loss: 1.1247 - val_acc: 0.6074\n",
      "Epoch 65/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.7734 - acc: 0.7159 - val_loss: 1.1082 - val_acc: 0.6180\n",
      "Epoch 66/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.7658 - acc: 0.7190 - val_loss: 1.1203 - val_acc: 0.6202\n",
      "Epoch 67/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.7626 - acc: 0.7194 - val_loss: 1.1343 - val_acc: 0.6102\n",
      "Epoch 68/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.7361 - acc: 0.7253 - val_loss: 1.1359 - val_acc: 0.6121\n",
      "Epoch 69/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.7310 - acc: 0.7316 - val_loss: 1.1295 - val_acc: 0.6135\n",
      "Epoch 70/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.7157 - acc: 0.7341 - val_loss: 1.1534 - val_acc: 0.6066\n",
      "Epoch 71/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.7019 - acc: 0.7407 - val_loss: 1.1520 - val_acc: 0.6113\n",
      "Epoch 72/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.6904 - acc: 0.7469 - val_loss: 1.1669 - val_acc: 0.6105\n",
      "Epoch 73/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.6832 - acc: 0.7514 - val_loss: 1.1671 - val_acc: 0.6119\n",
      "Epoch 74/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.6699 - acc: 0.7553 - val_loss: 1.1806 - val_acc: 0.6174\n",
      "Epoch 75/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.6568 - acc: 0.7601 - val_loss: 1.1821 - val_acc: 0.6160\n",
      "Epoch 76/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.6460 - acc: 0.7653 - val_loss: 1.1986 - val_acc: 0.6213\n",
      "Epoch 77/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.6342 - acc: 0.7684 - val_loss: 1.1894 - val_acc: 0.6186\n",
      "Epoch 78/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.6186 - acc: 0.7757 - val_loss: 1.2153 - val_acc: 0.6258\n",
      "Epoch 79/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.6110 - acc: 0.7730 - val_loss: 1.2080 - val_acc: 0.6252\n",
      "Epoch 80/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.6018 - acc: 0.7793 - val_loss: 1.2122 - val_acc: 0.6197\n",
      "Epoch 81/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.5822 - acc: 0.7863 - val_loss: 1.2175 - val_acc: 0.6216\n",
      "Epoch 82/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.5723 - acc: 0.7910 - val_loss: 1.2084 - val_acc: 0.6314\n",
      "Epoch 83/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.5617 - acc: 0.8001 - val_loss: 1.2231 - val_acc: 0.6197\n",
      "Epoch 84/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.5581 - acc: 0.7986 - val_loss: 1.2158 - val_acc: 0.6280\n",
      "Epoch 85/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.5423 - acc: 0.8030 - val_loss: 1.2350 - val_acc: 0.6252\n",
      "Epoch 86/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.5356 - acc: 0.8057 - val_loss: 1.2424 - val_acc: 0.6311\n",
      "Epoch 87/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.5211 - acc: 0.8101 - val_loss: 1.2339 - val_acc: 0.6291\n",
      "Epoch 88/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.5088 - acc: 0.8172 - val_loss: 1.2504 - val_acc: 0.6236\n",
      "Epoch 89/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.4920 - acc: 0.8214 - val_loss: 1.2588 - val_acc: 0.6219\n",
      "Epoch 90/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.4914 - acc: 0.8201 - val_loss: 1.2747 - val_acc: 0.6261\n",
      "Epoch 91/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.4864 - acc: 0.8250 - val_loss: 1.2729 - val_acc: 0.6219\n",
      "Epoch 92/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.4836 - acc: 0.8241 - val_loss: 1.2798 - val_acc: 0.6297\n",
      "Epoch 93/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.4674 - acc: 0.8298 - val_loss: 1.2932 - val_acc: 0.6286\n",
      "Epoch 94/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.4616 - acc: 0.8347 - val_loss: 1.2917 - val_acc: 0.6247\n",
      "Epoch 95/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.4622 - acc: 0.8343 - val_loss: 1.2963 - val_acc: 0.6227\n",
      "Epoch 96/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.4473 - acc: 0.8375 - val_loss: 1.3356 - val_acc: 0.6225\n",
      "Epoch 97/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.4392 - acc: 0.8428 - val_loss: 1.3345 - val_acc: 0.6266\n",
      "Epoch 98/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.4255 - acc: 0.8489 - val_loss: 1.3123 - val_acc: 0.6303\n",
      "Epoch 99/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.4182 - acc: 0.8499 - val_loss: 1.3559 - val_acc: 0.6222\n",
      "Epoch 100/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.4101 - acc: 0.8499 - val_loss: 1.3550 - val_acc: 0.6250\n",
      "Epoch 101/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.3983 - acc: 0.8550 - val_loss: 1.3813 - val_acc: 0.6305\n",
      "Epoch 102/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.3936 - acc: 0.8604 - val_loss: 1.4009 - val_acc: 0.6297\n",
      "Epoch 103/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.3937 - acc: 0.8589 - val_loss: 1.3982 - val_acc: 0.6294\n",
      "Epoch 104/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.3896 - acc: 0.8606 - val_loss: 1.3816 - val_acc: 0.6205\n",
      "Epoch 105/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.3732 - acc: 0.8684 - val_loss: 1.4071 - val_acc: 0.6275\n",
      "Epoch 106/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 9s 76ms/step - loss: 0.3651 - acc: 0.8677 - val_loss: 1.4176 - val_acc: 0.6194\n",
      "Epoch 107/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.3592 - acc: 0.8701 - val_loss: 1.4187 - val_acc: 0.6233\n",
      "Epoch 108/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.3518 - acc: 0.8751 - val_loss: 1.4323 - val_acc: 0.6213\n",
      "Epoch 109/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.3505 - acc: 0.8756 - val_loss: 1.3860 - val_acc: 0.6317\n",
      "Epoch 110/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.3529 - acc: 0.8741 - val_loss: 1.4674 - val_acc: 0.6314\n",
      "Epoch 111/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.3334 - acc: 0.8831 - val_loss: 1.4647 - val_acc: 0.6211\n",
      "Epoch 112/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.3371 - acc: 0.8794 - val_loss: 1.4625 - val_acc: 0.6311\n",
      "Epoch 113/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.3182 - acc: 0.8866 - val_loss: 1.4828 - val_acc: 0.6211\n",
      "Epoch 114/300\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 0.3288 - acc: 0.8831 - val_loss: 1.4699 - val_acc: 0.6294\n",
      "Epoch 115/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.3096 - acc: 0.8926 - val_loss: 1.4728 - val_acc: 0.6261\n",
      "Epoch 116/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.3199 - acc: 0.8866 - val_loss: 1.4938 - val_acc: 0.6280\n",
      "Epoch 117/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.3071 - acc: 0.8928 - val_loss: 1.5082 - val_acc: 0.6322\n",
      "Epoch 118/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2991 - acc: 0.8945 - val_loss: 1.4777 - val_acc: 0.6305\n",
      "Epoch 119/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2997 - acc: 0.8946 - val_loss: 1.4888 - val_acc: 0.6342\n",
      "Epoch 120/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.2899 - acc: 0.8986 - val_loss: 1.4793 - val_acc: 0.6344\n",
      "Epoch 121/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2899 - acc: 0.8972 - val_loss: 1.5389 - val_acc: 0.6317\n",
      "Epoch 122/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.2896 - acc: 0.8985 - val_loss: 1.5152 - val_acc: 0.6325\n",
      "Epoch 123/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2867 - acc: 0.8978 - val_loss: 1.4826 - val_acc: 0.6308\n",
      "Epoch 124/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2751 - acc: 0.9034 - val_loss: 1.5291 - val_acc: 0.6308\n",
      "Epoch 125/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.2782 - acc: 0.9026 - val_loss: 1.5289 - val_acc: 0.6264\n",
      "Epoch 126/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.2702 - acc: 0.9055 - val_loss: 1.5383 - val_acc: 0.6283\n",
      "Epoch 127/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.2718 - acc: 0.9042 - val_loss: 1.5435 - val_acc: 0.6211\n",
      "Epoch 128/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2625 - acc: 0.9068 - val_loss: 1.5534 - val_acc: 0.6255\n",
      "Epoch 129/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.2470 - acc: 0.9145 - val_loss: 1.5573 - val_acc: 0.6225\n",
      "Epoch 130/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2499 - acc: 0.9130 - val_loss: 1.5412 - val_acc: 0.6233\n",
      "Epoch 131/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.2515 - acc: 0.9115 - val_loss: 1.5530 - val_acc: 0.6147\n",
      "Epoch 132/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.2381 - acc: 0.9157 - val_loss: 1.6040 - val_acc: 0.6241\n",
      "Epoch 133/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2458 - acc: 0.9131 - val_loss: 1.5920 - val_acc: 0.6230\n",
      "Epoch 134/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.2399 - acc: 0.9164 - val_loss: 1.5661 - val_acc: 0.6191\n",
      "Epoch 135/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2303 - acc: 0.9196 - val_loss: 1.6374 - val_acc: 0.6227\n",
      "Epoch 136/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2317 - acc: 0.9191 - val_loss: 1.6101 - val_acc: 0.6311\n",
      "Epoch 137/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2290 - acc: 0.9196 - val_loss: 1.6041 - val_acc: 0.6278\n",
      "Epoch 138/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.2256 - acc: 0.9209 - val_loss: 1.6157 - val_acc: 0.6275\n",
      "Epoch 139/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.2222 - acc: 0.9233 - val_loss: 1.6572 - val_acc: 0.6241\n",
      "Epoch 140/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2157 - acc: 0.9238 - val_loss: 1.6561 - val_acc: 0.6350\n",
      "Epoch 141/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.2165 - acc: 0.9245 - val_loss: 1.6293 - val_acc: 0.6283\n",
      "Epoch 142/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2147 - acc: 0.9250 - val_loss: 1.6096 - val_acc: 0.6305\n",
      "Epoch 143/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.2244 - acc: 0.9222 - val_loss: 1.6330 - val_acc: 0.6264\n",
      "Epoch 144/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2106 - acc: 0.9272 - val_loss: 1.6135 - val_acc: 0.6314\n",
      "Epoch 145/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2028 - acc: 0.9301 - val_loss: 1.6441 - val_acc: 0.6347\n",
      "Epoch 146/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1965 - acc: 0.9302 - val_loss: 1.6726 - val_acc: 0.6239\n",
      "Epoch 147/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1982 - acc: 0.9306 - val_loss: 1.6665 - val_acc: 0.6241\n",
      "Epoch 148/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.2021 - acc: 0.9285 - val_loss: 1.6505 - val_acc: 0.6230\n",
      "Epoch 149/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1964 - acc: 0.9325 - val_loss: 1.6589 - val_acc: 0.6272\n",
      "Epoch 150/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1983 - acc: 0.9296 - val_loss: 1.6482 - val_acc: 0.6300\n",
      "Epoch 151/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.2042 - acc: 0.9302 - val_loss: 1.6666 - val_acc: 0.6297\n",
      "Epoch 152/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1913 - acc: 0.9338 - val_loss: 1.6715 - val_acc: 0.6286\n",
      "Epoch 153/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1938 - acc: 0.9329 - val_loss: 1.6995 - val_acc: 0.6239\n",
      "Epoch 154/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1818 - acc: 0.9366 - val_loss: 1.7032 - val_acc: 0.6325\n",
      "Epoch 155/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1909 - acc: 0.9347 - val_loss: 1.7080 - val_acc: 0.6311\n",
      "Epoch 156/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1870 - acc: 0.9343 - val_loss: 1.6672 - val_acc: 0.6286\n",
      "Epoch 157/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1815 - acc: 0.9378 - val_loss: 1.7052 - val_acc: 0.6269\n",
      "Epoch 158/300\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 0.1933 - acc: 0.9354 - val_loss: 1.6635 - val_acc: 0.6283\n",
      "Epoch 159/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1862 - acc: 0.9342 - val_loss: 1.6429 - val_acc: 0.6250\n",
      "Epoch 160/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1693 - acc: 0.9421 - val_loss: 1.7760 - val_acc: 0.6247\n",
      "Epoch 161/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1689 - acc: 0.9420 - val_loss: 1.7330 - val_acc: 0.6241\n",
      "Epoch 162/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1721 - acc: 0.9393 - val_loss: 1.7519 - val_acc: 0.6252\n",
      "Epoch 163/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1701 - acc: 0.9421 - val_loss: 1.7354 - val_acc: 0.6269\n",
      "Epoch 164/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1724 - acc: 0.9396 - val_loss: 1.7443 - val_acc: 0.6258\n",
      "Epoch 165/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1659 - acc: 0.9422 - val_loss: 1.7420 - val_acc: 0.6252\n",
      "Epoch 166/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1649 - acc: 0.9430 - val_loss: 1.7626 - val_acc: 0.6236\n",
      "Epoch 167/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1674 - acc: 0.9423 - val_loss: 1.7484 - val_acc: 0.6317\n",
      "Epoch 168/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1617 - acc: 0.9442 - val_loss: 1.7193 - val_acc: 0.6361\n",
      "Epoch 169/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1569 - acc: 0.9467 - val_loss: 1.8238 - val_acc: 0.6283\n",
      "Epoch 170/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1548 - acc: 0.9479 - val_loss: 1.7854 - val_acc: 0.6272\n",
      "Epoch 171/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1581 - acc: 0.9459 - val_loss: 1.8042 - val_acc: 0.6252\n",
      "Epoch 172/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1556 - acc: 0.9464 - val_loss: 1.7008 - val_acc: 0.6289\n",
      "Epoch 173/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1525 - acc: 0.9477 - val_loss: 1.7895 - val_acc: 0.6317\n",
      "Epoch 174/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.1543 - acc: 0.9468 - val_loss: 1.7852 - val_acc: 0.6325\n",
      "Epoch 175/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1519 - acc: 0.9472 - val_loss: 1.7911 - val_acc: 0.6275\n",
      "Epoch 176/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1583 - acc: 0.9443 - val_loss: 1.7704 - val_acc: 0.6397\n",
      "Epoch 177/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1443 - acc: 0.9513 - val_loss: 1.7635 - val_acc: 0.6330\n",
      "Epoch 178/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1450 - acc: 0.9491 - val_loss: 1.7824 - val_acc: 0.6261\n",
      "Epoch 179/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1451 - acc: 0.9507 - val_loss: 1.7374 - val_acc: 0.6278\n",
      "Epoch 180/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1466 - acc: 0.9500 - val_loss: 1.8144 - val_acc: 0.6211\n",
      "Epoch 181/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1514 - acc: 0.9475 - val_loss: 1.7997 - val_acc: 0.6269\n",
      "Epoch 182/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1413 - acc: 0.9523 - val_loss: 1.8415 - val_acc: 0.6344\n",
      "Epoch 183/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1340 - acc: 0.9541 - val_loss: 1.8338 - val_acc: 0.6294\n",
      "Epoch 184/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1402 - acc: 0.9517 - val_loss: 1.7808 - val_acc: 0.6319\n",
      "Epoch 185/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1398 - acc: 0.9536 - val_loss: 1.7683 - val_acc: 0.6358\n",
      "Epoch 186/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1364 - acc: 0.9531 - val_loss: 1.8026 - val_acc: 0.6392\n",
      "Epoch 187/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1404 - acc: 0.9514 - val_loss: 1.7704 - val_acc: 0.6342\n",
      "Epoch 188/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1371 - acc: 0.9541 - val_loss: 1.8040 - val_acc: 0.6375\n",
      "Epoch 189/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1362 - acc: 0.9540 - val_loss: 1.8058 - val_acc: 0.6344\n",
      "Epoch 190/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1421 - acc: 0.9520 - val_loss: 1.7705 - val_acc: 0.6383\n",
      "Epoch 191/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1331 - acc: 0.9545 - val_loss: 1.8304 - val_acc: 0.6350\n",
      "Epoch 192/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1272 - acc: 0.9562 - val_loss: 1.7821 - val_acc: 0.6367\n",
      "Epoch 193/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1334 - acc: 0.9542 - val_loss: 1.8116 - val_acc: 0.6330\n",
      "Epoch 194/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1321 - acc: 0.9556 - val_loss: 1.7887 - val_acc: 0.6336\n",
      "Epoch 195/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1212 - acc: 0.9582 - val_loss: 1.8486 - val_acc: 0.6339\n",
      "Epoch 196/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1290 - acc: 0.9569 - val_loss: 1.8433 - val_acc: 0.6350\n",
      "Epoch 197/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1277 - acc: 0.9570 - val_loss: 1.8644 - val_acc: 0.6261\n",
      "Epoch 198/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1301 - acc: 0.9562 - val_loss: 1.8504 - val_acc: 0.6364\n",
      "Epoch 199/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1227 - acc: 0.9572 - val_loss: 1.9033 - val_acc: 0.6372\n",
      "Epoch 200/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1281 - acc: 0.9558 - val_loss: 1.8614 - val_acc: 0.6445\n",
      "Epoch 201/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1235 - acc: 0.9579 - val_loss: 1.8801 - val_acc: 0.6414\n",
      "Epoch 202/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1204 - acc: 0.9583 - val_loss: 1.8892 - val_acc: 0.6350\n",
      "Epoch 203/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1233 - acc: 0.9588 - val_loss: 1.8829 - val_acc: 0.6300\n",
      "Epoch 204/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1186 - acc: 0.9609 - val_loss: 1.8703 - val_acc: 0.6417\n",
      "Epoch 205/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1169 - acc: 0.9606 - val_loss: 1.8546 - val_acc: 0.6397\n",
      "Epoch 206/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1163 - acc: 0.9605 - val_loss: 1.9184 - val_acc: 0.6308\n",
      "Epoch 207/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1231 - acc: 0.9589 - val_loss: 1.8364 - val_acc: 0.6333\n",
      "Epoch 208/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1215 - acc: 0.9588 - val_loss: 1.8023 - val_acc: 0.6305\n",
      "Epoch 209/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1183 - acc: 0.9590 - val_loss: 1.8637 - val_acc: 0.6367\n",
      "Epoch 210/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1180 - acc: 0.9608 - val_loss: 1.8827 - val_acc: 0.6353\n",
      "Epoch 211/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1203 - acc: 0.9583 - val_loss: 1.8492 - val_acc: 0.6255\n",
      "Epoch 212/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1176 - acc: 0.9611 - val_loss: 1.8787 - val_acc: 0.6308\n",
      "Epoch 213/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.1123 - acc: 0.9616 - val_loss: 1.8606 - val_acc: 0.6311\n",
      "Epoch 214/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.1147 - acc: 0.9615 - val_loss: 1.9297 - val_acc: 0.6328\n",
      "Epoch 215/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1108 - acc: 0.9624 - val_loss: 1.8925 - val_acc: 0.6342\n",
      "Epoch 216/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1110 - acc: 0.9628 - val_loss: 1.9133 - val_acc: 0.6325\n",
      "Epoch 217/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1083 - acc: 0.9641 - val_loss: 1.8841 - val_acc: 0.6330\n",
      "Epoch 218/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.1108 - acc: 0.9629 - val_loss: 1.9120 - val_acc: 0.6294\n",
      "Epoch 219/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1046 - acc: 0.9642 - val_loss: 1.8745 - val_acc: 0.6411\n",
      "Epoch 220/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1091 - acc: 0.9626 - val_loss: 1.9193 - val_acc: 0.6369\n",
      "Epoch 221/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1096 - acc: 0.9639 - val_loss: 1.8947 - val_acc: 0.6344\n",
      "Epoch 222/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1051 - acc: 0.9640 - val_loss: 1.9642 - val_acc: 0.6375\n",
      "Epoch 223/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.1045 - acc: 0.9646 - val_loss: 1.9362 - val_acc: 0.6336\n",
      "Epoch 224/300\n",
      "112/112 [==============================] - 9s 80ms/step - loss: 0.1075 - acc: 0.9620 - val_loss: 1.9179 - val_acc: 0.6361\n",
      "Epoch 225/300\n",
      "112/112 [==============================] - 9s 81ms/step - loss: 0.1053 - acc: 0.9635 - val_loss: 1.9359 - val_acc: 0.6330\n",
      "Epoch 226/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 9s 77ms/step - loss: 0.1065 - acc: 0.9632 - val_loss: 1.8614 - val_acc: 0.6333\n",
      "Epoch 227/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1045 - acc: 0.9648 - val_loss: 1.8656 - val_acc: 0.6325\n",
      "Epoch 228/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1116 - acc: 0.9632 - val_loss: 1.8462 - val_acc: 0.6308\n",
      "Epoch 229/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1033 - acc: 0.9635 - val_loss: 1.8930 - val_acc: 0.6350\n",
      "Epoch 230/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0982 - acc: 0.9670 - val_loss: 1.9286 - val_acc: 0.6311\n",
      "Epoch 231/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1034 - acc: 0.9661 - val_loss: 1.9309 - val_acc: 0.6294\n",
      "Epoch 232/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.1033 - acc: 0.9666 - val_loss: 1.9326 - val_acc: 0.6328\n",
      "Epoch 233/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.1006 - acc: 0.9667 - val_loss: 1.8860 - val_acc: 0.6344\n",
      "Epoch 234/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0965 - acc: 0.9675 - val_loss: 1.9588 - val_acc: 0.6280\n",
      "Epoch 235/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.0982 - acc: 0.9663 - val_loss: 1.9014 - val_acc: 0.6325\n",
      "Epoch 236/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.0973 - acc: 0.9675 - val_loss: 1.8996 - val_acc: 0.6325\n",
      "Epoch 237/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0970 - acc: 0.9664 - val_loss: 1.9053 - val_acc: 0.6325\n",
      "Epoch 238/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0947 - acc: 0.9678 - val_loss: 1.9329 - val_acc: 0.6330\n",
      "Epoch 239/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.1007 - acc: 0.9663 - val_loss: 1.8914 - val_acc: 0.6283\n",
      "Epoch 240/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0949 - acc: 0.9669 - val_loss: 1.9383 - val_acc: 0.6378\n",
      "Epoch 241/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0984 - acc: 0.9674 - val_loss: 1.9101 - val_acc: 0.6333\n",
      "Epoch 242/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0947 - acc: 0.9671 - val_loss: 1.9498 - val_acc: 0.6378\n",
      "Epoch 243/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0965 - acc: 0.9682 - val_loss: 1.9590 - val_acc: 0.6291\n",
      "Epoch 244/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1011 - acc: 0.9651 - val_loss: 1.9514 - val_acc: 0.6317\n",
      "Epoch 245/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0927 - acc: 0.9680 - val_loss: 1.9765 - val_acc: 0.6325\n",
      "Epoch 246/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0965 - acc: 0.9664 - val_loss: 1.9702 - val_acc: 0.6336\n",
      "Epoch 247/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0928 - acc: 0.9677 - val_loss: 1.9574 - val_acc: 0.6339\n",
      "Epoch 248/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0911 - acc: 0.9688 - val_loss: 1.9958 - val_acc: 0.6392\n",
      "Epoch 249/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.0970 - acc: 0.9685 - val_loss: 1.9315 - val_acc: 0.6367\n",
      "Epoch 250/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0920 - acc: 0.9686 - val_loss: 1.9501 - val_acc: 0.6353\n",
      "Epoch 251/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.0938 - acc: 0.9675 - val_loss: 1.9353 - val_acc: 0.6383\n",
      "Epoch 252/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0883 - acc: 0.9691 - val_loss: 1.9986 - val_acc: 0.6403\n",
      "Epoch 253/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0933 - acc: 0.9676 - val_loss: 1.9790 - val_acc: 0.6347\n",
      "Epoch 254/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0929 - acc: 0.9686 - val_loss: 2.0164 - val_acc: 0.6358\n",
      "Epoch 255/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0932 - acc: 0.9675 - val_loss: 1.9976 - val_acc: 0.6311\n",
      "Epoch 256/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.0886 - acc: 0.9704 - val_loss: 2.0000 - val_acc: 0.6369\n",
      "Epoch 257/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0838 - acc: 0.9720 - val_loss: 1.9651 - val_acc: 0.6367\n",
      "Epoch 258/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0853 - acc: 0.9715 - val_loss: 1.9761 - val_acc: 0.6381\n",
      "Epoch 259/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0884 - acc: 0.9692 - val_loss: 1.9595 - val_acc: 0.6375\n",
      "Epoch 260/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0923 - acc: 0.9682 - val_loss: 1.9457 - val_acc: 0.6381\n",
      "Epoch 261/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.0898 - acc: 0.9698 - val_loss: 1.9336 - val_acc: 0.6344\n",
      "Epoch 262/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0814 - acc: 0.9725 - val_loss: 2.0073 - val_acc: 0.6350\n",
      "Epoch 263/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.0837 - acc: 0.9715 - val_loss: 2.0083 - val_acc: 0.6381\n",
      "Epoch 264/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0847 - acc: 0.9705 - val_loss: 2.0168 - val_acc: 0.6342\n",
      "Epoch 265/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0849 - acc: 0.9722 - val_loss: 2.0154 - val_acc: 0.6386\n",
      "Epoch 266/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0879 - acc: 0.9696 - val_loss: 2.0196 - val_acc: 0.6383\n",
      "Epoch 267/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.0834 - acc: 0.9716 - val_loss: 1.9831 - val_acc: 0.6350\n",
      "Epoch 268/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0903 - acc: 0.9693 - val_loss: 1.9730 - val_acc: 0.6367\n",
      "Epoch 269/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0778 - acc: 0.9738 - val_loss: 1.9822 - val_acc: 0.6431\n",
      "Epoch 270/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0823 - acc: 0.9725 - val_loss: 1.9943 - val_acc: 0.6300\n",
      "Epoch 271/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.0803 - acc: 0.9726 - val_loss: 2.0236 - val_acc: 0.6286\n",
      "Epoch 272/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.0780 - acc: 0.9735 - val_loss: 1.9769 - val_acc: 0.6367\n",
      "Epoch 273/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0817 - acc: 0.9721 - val_loss: 1.9853 - val_acc: 0.6439\n",
      "Epoch 274/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0821 - acc: 0.9710 - val_loss: 1.9945 - val_acc: 0.6403\n",
      "Epoch 275/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0797 - acc: 0.9726 - val_loss: 1.9740 - val_acc: 0.6436\n",
      "Epoch 276/300\n",
      "112/112 [==============================] - 9s 77ms/step - loss: 0.0793 - acc: 0.9716 - val_loss: 2.0128 - val_acc: 0.6400\n",
      "Epoch 277/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0747 - acc: 0.9743 - val_loss: 2.0593 - val_acc: 0.6381\n",
      "Epoch 278/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0806 - acc: 0.9720 - val_loss: 1.9847 - val_acc: 0.6372\n",
      "Epoch 279/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0741 - acc: 0.9744 - val_loss: 2.0158 - val_acc: 0.6392\n",
      "Epoch 280/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0784 - acc: 0.9729 - val_loss: 1.9914 - val_acc: 0.6375\n",
      "Epoch 281/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0802 - acc: 0.9725 - val_loss: 1.9869 - val_acc: 0.6356\n",
      "Epoch 282/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0797 - acc: 0.9727 - val_loss: 2.0541 - val_acc: 0.6381\n",
      "Epoch 283/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.0803 - acc: 0.9730 - val_loss: 2.0439 - val_acc: 0.6372\n",
      "Epoch 284/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0831 - acc: 0.9719 - val_loss: 2.0307 - val_acc: 0.6303\n",
      "Epoch 285/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0805 - acc: 0.9729 - val_loss: 2.0369 - val_acc: 0.6314\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0746 - acc: 0.9756 - val_loss: 2.0883 - val_acc: 0.6291\n",
      "Epoch 287/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0875 - acc: 0.9704 - val_loss: 2.0304 - val_acc: 0.6294\n",
      "Epoch 288/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0823 - acc: 0.9725 - val_loss: 1.9992 - val_acc: 0.6342\n",
      "Epoch 289/300\n",
      "112/112 [==============================] - 8s 76ms/step - loss: 0.0807 - acc: 0.9723 - val_loss: 2.0203 - val_acc: 0.6406\n",
      "Epoch 290/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0760 - acc: 0.9732 - val_loss: 2.0142 - val_acc: 0.6317\n",
      "Epoch 291/300\n",
      "112/112 [==============================] - 9s 76ms/step - loss: 0.0728 - acc: 0.9759 - val_loss: 2.0649 - val_acc: 0.6367\n",
      "Epoch 292/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.0781 - acc: 0.9738 - val_loss: 2.0510 - val_acc: 0.6367\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0761 - acc: 0.9744 - val_loss: 2.0515 - val_acc: 0.6364\n",
      "Epoch 294/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0710 - acc: 0.9749 - val_loss: 2.0450 - val_acc: 0.6392\n",
      "Epoch 295/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0739 - acc: 0.9753 - val_loss: 2.0596 - val_acc: 0.6317\n",
      "Epoch 296/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0684 - acc: 0.9766 - val_loss: 2.1018 - val_acc: 0.6311\n",
      "Epoch 297/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0752 - acc: 0.9749 - val_loss: 2.0490 - val_acc: 0.6339\n",
      "Epoch 298/300\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.0747 - acc: 0.9744 - val_loss: 2.0359 - val_acc: 0.6378\n",
      "Epoch 299/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0718 - acc: 0.9757 - val_loss: 2.0583 - val_acc: 0.6392\n",
      "Epoch 300/300\n",
      "112/112 [==============================] - 8s 75ms/step - loss: 0.0710 - acc: 0.9758 - val_loss: 2.0875 - val_acc: 0.6372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e3a808080>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generator           = train_generator,\n",
    "    validation_data=(X_dev, Y_dev), \n",
    "    steps_per_epoch=len(X_train) // BS,\n",
    "    shuffle=True,\n",
    "    epochs=EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iKzZuMBafv0a",
    "outputId": "6ee6a4b5-e98e-4f4a-acbe-5ed67ba48de9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(X_train,y_train,batch_size=500,epochs=1000,validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FSBxRPXHSZE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "3589/3589 [==============================] - 0s 61us/step\n",
      "test loss, test acc: [2.087482804440296, 0.6372248537280029]\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on dev data')\n",
    "results_dev = model.evaluate(X_dev, Y_dev , batch_size=128)\n",
    "print('dev loss, dev acc:', results_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "3589/3589 [==============================] - 0s 39us/step\n",
      "test loss, test acc: [1.9984358973608072, 0.6386179994759569]\n"
     ]
    }
   ],
   "source": [
    "test_dataset_dir = 'fer2013/test.csv'\n",
    "X_test, Y_test      = get_data(test_dataset_dir)\n",
    "\n",
    "print('\\n# Evaluate on test data')\n",
    "results_test = model.evaluate(X_test, Y_test , batch_size=128)\n",
    "print('test loss, test acc:', results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_str = '-SGD_LR_%.5f' % SGD_LEARNING_RATE\n",
    "epoch_str = '-EPOCHS_' + str(EPOCHS)\n",
    "bs_str = '-BS_' + str(BS)\n",
    "dropout_str = '-DROPOUT_' + str(DROPOUT_RATE)\n",
    "model.save('models/soa' + lr_str + epoch_str + bs_str + dropout_str + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fer2013.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
