{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fer2013.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwdg7Sv3XBaP",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2nz38mJZXN_P",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EjKPXZ3TX3Jb",
        "outputId": "7e1140f2-c1f7-403d-f04d-17efdc4ecfc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv('/content/drive/My Drive/cs230 project/collab/fer2013/fer2013.csv')\n",
        "\n",
        "#print('Number of samples in the dataset: ', data.shape[0])\n",
        "# Transform images from strings to lists of integers. TODO: use an array cast\n",
        "data['pixels'] = data['pixels'].apply(lambda x: [int(pixel) for pixel in x.split()])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oXIX_VLMYJty",
        "colab": {}
      },
      "source": [
        "emotion_cat = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}\n",
        "\n",
        "# See the target distribution (check for imbalance)\n",
        "#target_counts = data['emotion'].value_counts().reset_index(drop=False)\n",
        "#target_counts.columns = ['emotion', 'number_samples']\n",
        "#target_counts['emotion'] = target_counts['emotion'].map(emotion_cat)\n",
        "#target_counts\n",
        "\n",
        "# Select randomly 10 images\n",
        "#random_seed = 1\n",
        "#data_sample = data.sample(10, random_state=random_seed)\n",
        "#f, axarr = plt.subplots(2, 5, figsize=(20, 10))\n",
        "\n",
        "#i, j = 0, 0\n",
        "#for idx, row in data_sample.iterrows():\n",
        "#    img = np.array(row['pixels']).reshape(48,48)\n",
        "#    axarr[i,j].imshow(img, cmap='gray')\n",
        "#    axarr[i,j].set_title(emotion_cat[row['emotion']])\n",
        "#    if j==4:\n",
        "#        i += 1\n",
        "#        j = 0\n",
        "#    else:\n",
        "#        j += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hAjh6yOLYPZm",
        "colab": {}
      },
      "source": [
        "data_train = data[data['Usage']=='Training']\n",
        "#print('Number samples in the training dataset: ', data_train.shape[0])\n",
        "\n",
        "data_dev = data[data['Usage']=='PublicTest']\n",
        "#print('Number samples in the development dataset: ', data_dev.shape[0])\n",
        "\n",
        "# Retrieve train input and target\n",
        "X_train, y_train = data_train['pixels'].tolist(), data_train['emotion'].values\n",
        "# Reshape images to 4D (num_samples, width, height, num_channels)\n",
        "X_train = np.array(X_train, dtype='float32').reshape(-1,48,48,1)\n",
        "# Normalize images with max (the maximum pixel intensity is 255)\n",
        "X_train = X_train/255.0\n",
        "\n",
        "# Retrieve dev input and target\n",
        "X_dev, y_dev = data_dev['pixels'].tolist(), data_dev['emotion'].values\n",
        "X_dev = np.array(X_dev, dtype='float32').reshape(-1,48,48,1)\n",
        "X_dev = X_dev/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yi3lopGhZIuT",
        "colab": {}
      },
      "source": [
        "#CPNCPNCPCFF 6 layers, 21.3m params, 75% state of art acc https://arxiv.org/pdf/1612.02903.pdf\n",
        "model = tf.keras.models.Sequential(   \n",
        "    [\n",
        "      InputLayer(input_shape=(48,48,1),name=\"input\"),\n",
        "      Conv2D(filters=64,kernel_size=5,activation='relu',padding='same',name=\"conv1\"),\n",
        "      MaxPool2D(pool_size=(2,2),name=\"maxpool1\"),\n",
        "      BatchNormalization(),\n",
        "      Conv2D(filters=96,kernel_size=5,activation='relu',padding='same',name=\"conv2\"),\n",
        "      MaxPool2D(pool_size=(2,2),name=\"maxpool2\"),\n",
        "      BatchNormalization(),\n",
        "      Conv2D(filters=256,kernel_size=5,activation='relu',padding='same',name=\"conv3\"),\n",
        "      MaxPool2D(pool_size=(2,2),name=\"maxpool3\"),\n",
        "      BatchNormalization(),\n",
        "      Conv2D(filters=256,kernel_size=5,activation='relu',padding='same',name=\"conv4\"),\n",
        "      BatchNormalization(),\n",
        "      Flatten(),\n",
        "      Dense(2048,input_shape=(6*6*256,1),activation='relu',name='fc1'),\n",
        "      BatchNormalization(),\n",
        "      Dense(7,input_shape=(2048,1),activation='softmax',name='fc-softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iKzZuMBafv0a",
        "scrolled": true,
        "outputId": "71850318-bf27-4051-9269-d9f575f1fe2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.fit(X_train,y_train,batch_size=32,epochs=20,validation_data=(X_dev, y_dev))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/20\n",
            "28709/28709 [==============================] - 16s 563us/sample - loss: 1.5161 - accuracy: 0.4478 - val_loss: 1.7416 - val_accuracy: 0.4040\n",
            "Epoch 2/20\n",
            "28709/28709 [==============================] - 16s 558us/sample - loss: 1.3910 - accuracy: 0.5088 - val_loss: 1.3521 - val_accuracy: 0.4739\n",
            "Epoch 3/20\n",
            "28709/28709 [==============================] - 16s 565us/sample - loss: 1.2641 - accuracy: 0.5579 - val_loss: 1.4997 - val_accuracy: 0.5026\n",
            "Epoch 4/20\n",
            "28709/28709 [==============================] - 16s 564us/sample - loss: 1.1149 - accuracy: 0.6191 - val_loss: 1.4092 - val_accuracy: 0.5155\n",
            "Epoch 5/20\n",
            "28709/28709 [==============================] - 16s 568us/sample - loss: 0.9588 - accuracy: 0.6819 - val_loss: 1.2840 - val_accuracy: 0.5313\n",
            "Epoch 6/20\n",
            "28709/28709 [==============================] - 16s 558us/sample - loss: 0.6443 - accuracy: 0.7709 - val_loss: 1.9568 - val_accuracy: 0.5587\n",
            "Epoch 7/20\n",
            "28709/28709 [==============================] - 16s 561us/sample - loss: 0.4510 - accuracy: 0.8422 - val_loss: 1.8544 - val_accuracy: 0.5481\n",
            "Epoch 8/20\n",
            "28709/28709 [==============================] - 16s 567us/sample - loss: 0.2836 - accuracy: 0.9047 - val_loss: 1.7828 - val_accuracy: 0.5662\n",
            "Epoch 9/20\n",
            "28709/28709 [==============================] - 16s 562us/sample - loss: 0.2197 - accuracy: 0.9256 - val_loss: 2.0338 - val_accuracy: 0.5790\n",
            "Epoch 10/20\n",
            "28709/28709 [==============================] - 16s 561us/sample - loss: 0.1853 - accuracy: 0.9393 - val_loss: 2.2910 - val_accuracy: 0.5559\n",
            "Epoch 11/20\n",
            "28709/28709 [==============================] - 16s 562us/sample - loss: 0.1663 - accuracy: 0.9444 - val_loss: 2.4085 - val_accuracy: 0.5339\n",
            "Epoch 12/20\n",
            "28709/28709 [==============================] - 16s 565us/sample - loss: 0.1357 - accuracy: 0.9545 - val_loss: 2.2970 - val_accuracy: 0.5860\n",
            "Epoch 13/20\n",
            "28709/28709 [==============================] - 16s 561us/sample - loss: 0.1264 - accuracy: 0.9579 - val_loss: 2.4031 - val_accuracy: 0.5874\n",
            "Epoch 14/20\n",
            "28709/28709 [==============================] - 16s 564us/sample - loss: 0.1330 - accuracy: 0.9550 - val_loss: 27.6737 - val_accuracy: 0.5634\n",
            "Epoch 15/20\n",
            "28709/28709 [==============================] - 16s 563us/sample - loss: 0.1144 - accuracy: 0.9614 - val_loss: 2.2176 - val_accuracy: 0.5809\n",
            "Epoch 16/20\n",
            "28709/28709 [==============================] - 16s 562us/sample - loss: 0.1220 - accuracy: 0.9603 - val_loss: 2.5324 - val_accuracy: 0.5600\n",
            "Epoch 17/20\n",
            "28709/28709 [==============================] - 16s 561us/sample - loss: 0.0764 - accuracy: 0.9751 - val_loss: 2.7265 - val_accuracy: 0.5765\n",
            "Epoch 18/20\n",
            "28709/28709 [==============================] - 16s 560us/sample - loss: 0.0878 - accuracy: 0.9703 - val_loss: 22.9088 - val_accuracy: 0.4868\n",
            "Epoch 19/20\n",
            "28709/28709 [==============================] - 16s 561us/sample - loss: 0.0826 - accuracy: 0.9735 - val_loss: 2.5331 - val_accuracy: 0.5913\n",
            "Epoch 20/20\n",
            "28709/28709 [==============================] - 16s 561us/sample - loss: 0.0998 - accuracy: 0.9667 - val_loss: 2.6388 - val_accuracy: 0.5790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe5b601ea20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSBxRPXHSZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}