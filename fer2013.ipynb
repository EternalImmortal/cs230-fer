{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/amilkh/cs230-fer/blob/transfer-learning/fer2013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwdg7Sv3XBaP"
   },
   "outputs": [],
   "source": [
    "#tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nz38mJZXN_P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import *\n",
    "from keras.engine import Model\n",
    "from keras.layers import Input, Flatten, Dense, Activation, Conv2D, MaxPool2D, BatchNormalization, Dropout, MaxPooling2D\n",
    "from skimage.transform import rescale, resize\n",
    "\n",
    "import pydot\n",
    "\n",
    "#!python -c \"import skimage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try:\n",
    "    pydot.Dot.create(pydot.Dot())\n",
    "except:\n",
    "    print('pydot error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nUcd6yIGduUW",
    "outputId": "94e06002-1afb-4fab-b6fc-f69cef85fa1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_csv('fer2013/fer2013.csv')\n",
    "data.head()\n",
    "\n",
    "data_train = data[data['Usage'] == 'Training']\n",
    "#print('Number samples in the training dataset: ', data_train.shape[0])\n",
    "\n",
    "data_dev = data[data['Usage'] == 'PublicTest']\n",
    "#print('Number samples in the development dataset: ', data_dev.shape[0])\n",
    "data_dev.head()\n",
    "\n",
    "data_train.to_csv('fer2013/train.csv')\n",
    "data_dev.to_csv('fer2013/dev.csv')\n",
    "\n",
    "print(data_train.shape)\n",
    "\n",
    "print(data_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_csv('fer2013/icml_face_data.csv') \n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "print(data[' Usage'].unique())\n",
    "\n",
    "data_dev2 = data[data[' Usage'] == 'PublicTest']\n",
    "#print('Number samples in the development dataset: ', data_dev.shape[0])\n",
    "print(data_dev2.head())\n",
    "\n",
    "data_dev2.to_csv('fer2013/dev2.csv')\n",
    "print(data_dev2.shape)\n",
    "\n",
    "data_test = data[data[' Usage'] == 'PrivateTest']\n",
    "#print('Number samples in the development dataset: ', data_dev.shape[0])\n",
    "print(data_test.head())\n",
    "\n",
    "data_test.to_csv('fer2013/test.csv')\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EarfWgg_Y4rR",
    "outputId": "a053f8b0-3c50-46bd-abc3-19e52a91aed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f76d77bc390> [0]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d77bc6d8> [1]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76d77bc860> [2]\n",
      "<keras.layers.core.Activation object at 0x7f76d9585860> [3]\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f77049a1ef0> [4]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d46f3080> [5]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76d467f0b8> [6]\n",
      "<keras.layers.core.Activation object at 0x7f76d467ffd0> [7]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d464d978> [8]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76d4631eb8> [9]\n",
      "<keras.layers.core.Activation object at 0x7f76d45f1518> [10]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d44e5fd0> [11]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d4504390> [12]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76d4542a90> [13]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76d443c9e8> [14]\n",
      "<keras.layers.merge.Add object at 0x7f76d4392a58> [15]\n",
      "<keras.layers.core.Activation object at 0x7f76d43d2a90> [16]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d43133c8> [17]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76d42b6160> [18]\n",
      "<keras.layers.core.Activation object at 0x7f76d4282978> [19]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d4251390> [20]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76d41a32e8> [21]\n",
      "<keras.layers.core.Activation object at 0x7f76d41c8160> [22]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d4115fd0> [23]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76d40f8dd8> [24]\n",
      "<keras.layers.merge.Add object at 0x7f76d40b9dd8> [25]\n",
      "<keras.layers.core.Activation object at 0x7f76d4059be0> [26]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76d40590b8> [27]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ce7a5ba8> [28]\n",
      "<keras.layers.core.Activation object at 0x7f76ce7202e8> [29]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ce7cfc88> [30]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ce6f5940> [31]\n",
      "<keras.layers.core.Activation object at 0x7f76ce6bc390> [32]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ce5d69b0> [33]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ce5ac908> [34]\n",
      "<keras.layers.merge.Add object at 0x7f76ce54d630> [35]\n",
      "<keras.layers.core.Activation object at 0x7f76ce4c89b0> [36]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ce444630> [37]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ce4663c8> [38]\n",
      "<keras.layers.core.Activation object at 0x7f76ce433ba8> [39]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ce3fc470> [40]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ce358518> [41]\n",
      "<keras.layers.core.Activation object at 0x7f76ce378240> [42]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ce2982e8> [43]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ce26f630> [44]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ce2af2e8> [45]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ce13d6a0> [46]\n",
      "<keras.layers.merge.Add object at 0x7f76ce09f1d0> [47]\n",
      "<keras.layers.core.Activation object at 0x7f76ce131c88> [48]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ce0533c8> [49]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ce015860> [50]\n",
      "<keras.layers.core.Activation object at 0x7f76cdf8fb00> [51]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cdfae358> [52]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cdf07898> [53]\n",
      "<keras.layers.core.Activation object at 0x7f76cdf275c0> [54]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cde436a0> [55]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cde1e5f8> [56]\n",
      "<keras.layers.merge.Add object at 0x7f76cddbf320> [57]\n",
      "<keras.layers.core.Activation object at 0x7f76cddb56a0> [58]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cdd30320> [59]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cdcd30b8> [60]\n",
      "<keras.layers.core.Activation object at 0x7f76cdca3898> [61]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cdc514a8> [62]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cdc082b0> [63]\n",
      "<keras.layers.core.Activation object at 0x7f76cdbc85f8> [64]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cdb33ef0> [65]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cdb16cf8> [66]\n",
      "<keras.layers.merge.Add object at 0x7f76cda50c88> [67]\n",
      "<keras.layers.core.Activation object at 0x7f76cda771d0> [68]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cda77cf8> [69]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd9efac8> [70]\n",
      "<keras.layers.core.Activation object at 0x7f76cd967e48> [71]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd906438> [72]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd8e1b00> [73]\n",
      "<keras.layers.core.Activation object at 0x7f76cd8848d0> [74]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd81a898> [75]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd7f3828> [76]\n",
      "<keras.layers.merge.Add object at 0x7f76cd790550> [77]\n",
      "<keras.layers.core.Activation object at 0x7f76cd70e8d0> [78]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd689550> [79]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd6aa2e8> [80]\n",
      "<keras.layers.core.Activation object at 0x7f76cd67ab00> [81]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd5c94e0> [82]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd5e1748> [83]\n",
      "<keras.layers.core.Activation object at 0x7f76cd53e2b0> [84]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd532978> [85]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd4b0e10> [86]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd4f1208> [87]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd3825c0> [88]\n",
      "<keras.layers.merge.Add object at 0x7f76cd2e36a0> [89]\n",
      "<keras.layers.core.Activation object at 0x7f76cd375908> [90]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd332b00> [91]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd258be0> [92]\n",
      "<keras.layers.core.Activation object at 0x7f76cd1ca908> [93]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd196320> [94]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd1ae358> [95]\n",
      "<keras.layers.core.Activation object at 0x7f76cd169668> [96]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cd05bf98> [97]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd03eda0> [98]\n",
      "<keras.layers.merge.Add object at 0x7f76ccfeffd0> [99]\n",
      "<keras.layers.core.Activation object at 0x7f76ccf9c278> [100]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ccf9c080> [101]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cd01dac8> [102]\n",
      "<keras.layers.core.Activation object at 0x7f76cce80390> [103]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cce70978> [104]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cce5ae48> [105]\n",
      "<keras.layers.core.Activation object at 0x7f76cce14f28> [106]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ccd79c88> [107]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ccd56b70> [108]\n",
      "<keras.layers.merge.Add object at 0x7f76ccd1ac88> [109]\n",
      "<keras.layers.core.Activation object at 0x7f76cccb4b70> [110]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ccc69470> [111]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ccc2f908> [112]\n",
      "<keras.layers.core.Activation object at 0x7f76ccb9e588> [113]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ccb1eeb8> [114]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ccb7ccc0> [115]\n",
      "<keras.layers.core.Activation object at 0x7f76ccabaf60> [116]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cca34da0> [117]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cca11978> [118]\n",
      "<keras.layers.merge.Add object at 0x7f76cc9d7a58> [119]\n",
      "<keras.layers.core.Activation object at 0x7f76cc94e198> [120]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc905908> [121]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc8e66a0> [122]\n",
      "<keras.layers.core.Activation object at 0x7f76cc85d438> [123]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc7d9c50> [124]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc836b38> [125]\n",
      "<keras.layers.core.Activation object at 0x7f76cc7f9c50> [126]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc7169b0> [127]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc6cb438> [128]\n",
      "<keras.layers.merge.Add object at 0x7f76cc690940> [129]\n",
      "<keras.layers.core.Activation object at 0x7f76cc609b00> [130]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc57f780> [131]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc59e518> [132]\n",
      "<keras.layers.core.Activation object at 0x7f76cc6a5198> [133]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc492d68> [134]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc4ef940> [135]\n",
      "<keras.layers.core.Activation object at 0x7f76cc4b5390> [136]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc3cc978> [137]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc3a58d0> [138]\n",
      "<keras.layers.merge.Add object at 0x7f76cc3475f8> [139]\n",
      "<keras.layers.core.Activation object at 0x7f76cc2c2978> [140]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc2bb5f8> [141]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc25d358> [142]\n",
      "<keras.layers.core.Activation object at 0x7f76cc1d2da0> [143]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc1f2390> [144]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc1a97b8> [145]\n",
      "<keras.layers.core.Activation object at 0x7f76cc16c940> [146]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76cc0867f0> [147]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76abfc1470> [148]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76cc05d748> [149]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76abed9240> [150]\n",
      "<keras.layers.merge.Add object at 0x7f76abf385c0> [151]\n",
      "<keras.layers.core.Activation object at 0x7f76abe51b70> [152]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76abdd0438> [153]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76abdf11d0> [154]\n",
      "<keras.layers.core.Activation object at 0x7f76abd69ac8> [155]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76abd08978> [156]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76abce0898> [157]\n",
      "<keras.layers.core.Activation object at 0x7f76abcff588> [158]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76abc1b668> [159]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76abbf7c50> [160]\n",
      "<keras.layers.merge.Add object at 0x7f76abb984a8> [161]\n",
      "<keras.layers.core.Activation object at 0x7f76abb0e630> [162]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76abb0eac8> [163]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76abaaa128> [164]\n",
      "<keras.layers.core.Activation object at 0x7f76aba7bdd8> [165]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76aba229e8> [166]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ab998710> [167]\n",
      "<keras.layers.core.Activation object at 0x7f76ab9b8438> [168]\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f76ab8d34e0> [169]\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f76ab8af438> [170]\n",
      "<keras.layers.merge.Add object at 0x7f76ab84d208> [171]\n",
      "<keras.layers.core.Activation object at 0x7f76ab7cf6a0> [172]\n",
      "<keras.layers.pooling.AveragePooling2D object at 0x7f76ab7cf4a8> [173]\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7f76ab783be0> [174]\n",
      "vgg layer 2 is trainable: True\n",
      "vgg layer 3 is trainable: False\n"
     ]
    }
   ],
   "source": [
    "Resize_pixelsize = 197\n",
    "vgg_notop = VGGFace(model='resnet50', include_top=False, input_shape=(Resize_pixelsize, Resize_pixelsize, 3), pooling='avg')\n",
    "last_layer = vgg_notop.get_layer('avg_pool').output\n",
    "x = Flatten(name='flatten')(last_layer)\n",
    "x = Dense(4096, activation='relu', name='fc6')(x)\n",
    "x = Dense(1024, activation='relu', name='fc7')(x)\n",
    "#print(\"Emotions count\", len(EMOTIONS))\n",
    "l=0\n",
    "for layer in vgg_notop.layers:\n",
    "    print(layer,\"[\"+str(l)+\"]\")\n",
    "    l=l+1\n",
    "    \n",
    "batch_norm_indices = [2, 6, 9, 13, 14, 18, 21, 24, 28, 31, 34, 38, 41, 45, 46, 53, 56, 60, 63, 66, 70, 73, 76, 80, 83, 87, 88, 92, 95, 98]\n",
    "for i in range(101):\n",
    "    if i not in batch_norm_indices:\n",
    "        vgg_notop.layers[i].trainable = False\n",
    "\n",
    "print('vgg layer 2 is trainable: ' + str(vgg_notop.layers[2].trainable))\n",
    "print('vgg layer 3 is trainable: ' + str(vgg_notop.layers[3].trainable))\n",
    "\n",
    "out = Dense(7, activation='softmax', name='classifier')(x)\n",
    "\n",
    "custom_resnet = Model(vgg_notop.input, out)\n",
    "\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#optim = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "custom_resnet.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "plot_model(custom_resnet, to_file='model2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EjKPXZ3TX3Jb",
    "outputId": "ee0e13f9-5570-4876-f3b8-ac0ca4a7818e"
   },
   "outputs": [],
   "source": [
    "Batch_no = 0\n",
    "# Function that reads the data from the csv file, increases the size of the images and returns the images and their labels\n",
    "    # dataset: Data path\n",
    "def get_data(datas, pixelsize = Resize_pixelsize, bs= 32, aug=None):\n",
    "    global Batch_no\n",
    "    # Data preparation\n",
    "    file_stream = file_io.FileIO(training_dataset_dir, mode='r')\n",
    "    datas = pd.read_csv(file_stream,iterator=True, chunksize=bs )\n",
    "    for data in datas:\n",
    "        #data = pd.read_csv('fer2013/fer2013.csv')\n",
    "        data['pixels'] = data['pixels'].apply(lambda x: [int(pixel) for pixel in x.split()])\n",
    "\n",
    "        # Retrieve train input and target\n",
    "        X, Y = data['pixels'].tolist(), data['emotion'].values\n",
    "        #print(len(X))\n",
    "        #print(X[0])\n",
    "        # Reshape images to 4D (num_samples, width, height, num_channels)\n",
    "        X = np.array(X, dtype='float32').reshape(-1,48,48,1)\n",
    "        # Normalize images with max (the maximum pixel intensity is 255)\n",
    "        X = X/255.0\n",
    "        #print(X.shape)\n",
    "        #print(X[0])\n",
    "        #image_resized = resize(image, (image.shape[0] // 4, image.shape[1] // 4), anti_aliasing=True)\n",
    "\n",
    "        X_res = np.zeros((X.shape[0], pixelsize,pixelsize,3))\n",
    "        for ind in range(X.shape[0]):  #X_dev.shape[0]\n",
    "            sample = X[ind]\n",
    "            sample = sample.reshape(48, 48)\n",
    "            #plt.imshow(sample, cmap='gray')\n",
    "            #plt.show()\n",
    "            image_resized = resize(sample, (pixelsize, pixelsize), anti_aliasing=True)\n",
    "            X_res[ind,:,:,:] = image_resized.reshape(pixelsize,pixelsize,1)\n",
    "        #         np.save('X_res.npy', X_res)\n",
    "\n",
    "        Y_res = np.zeros((Y.size, 7))\n",
    "        Y_res[np.arange(Y.size),Y] = 1\n",
    "        # if the data augmentation object is not None, apply it\n",
    "        if aug is not None:\n",
    "            (X_res, Y_res) = next(aug.flow(np.array(X_res),\n",
    "                Y_res, batch_size=bs))\n",
    "        # if the data augmentation object is not None, apply it\n",
    "        print(Batch_no)\n",
    "        Batch_no += 1\n",
    "        yield  X_res, Y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BS = 128\n",
    "training_dataset_dir = 'fer2013/train.csv'\n",
    "dev_dataset_dir = 'fer2013/dev.csv'\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range  = 10,\n",
    "    shear_range     = 10, # 10 degrees\n",
    "    zoom_range      = 0.1,\n",
    "    fill_mode       = 'reflect',\n",
    "    horizontal_flip = True)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = get_data(training_dataset_dir, Resize_pixelsize, bs=BS, aug=aug)\n",
    "dev_generator   = get_data(dev_dataset_dir, Resize_pixelsize, bs=BS, aug=None)\n",
    "    #X_dev_res, Y_dev_res  = get_data(dev_dataset_dir)\n",
    "\n",
    "# Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely\n",
    "# rescale:          Rescaling factor (defaults to None). Multiply the data by the value provided (before applying any other transformation)\n",
    "# rotation_range:   Int. Degree range for random rotations\n",
    "# shear_range:      Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "# zoom_range:       Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]\n",
    "# fill_mode :       Points outside the boundaries of the input are filled according to the given mode: {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}\n",
    "# horizontal_flip:  Boolean. Randomly flip inputs horizontally\n",
    "\n",
    "\n",
    "# Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batcfillhes indefinitely, in an infinite loop\n",
    "    # x:            Data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, \n",
    "    #               it should have value 3\n",
    "    # y:            Labels\n",
    "    # batch_size:   Int (default: 32)\n",
    "#train_generator = train_datagen.flow(X_train_res, Y_train_res,  batch_size  = BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"X_train shape: \" + str(X_train.shape))\n",
    "#print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "#print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "#print (\"Y_dev shape: \" + str(Y_dev.shape))\n",
    "\n",
    "print (\"X_train_res shape: \" + str(X_train_res.shape))\n",
    "print(\"Y_train_res shape: \" + str(Y_train_res.shape))\n",
    "print (\"X_dev_res shape: \" + str(X_dev_res.shape))\n",
    "print(\"Y_dev_res shape: \" + str(Y_dev_res.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3DpD608HuiQ"
   },
   "outputs": [],
   "source": [
    "i = 10\n",
    "#image = X_train[i,:,:,:].reshape(48, 48)\n",
    "#plt.imshow(image, cmap='gray')\n",
    "#plt.show()\n",
    "\n",
    "image_resized = X_train_res[i,:,:,:].reshape(Resize_pixelsize, Resize_pixelsize, 3)\n",
    "plt.imshow(image_resized)\n",
    "plt.show()\n",
    "print(Y_train_res[i])\n",
    "\n",
    "dev_resized = X_dev_res[i,:,:,:].reshape(Resize_pixelsize, Resize_pixelsize, 3)\n",
    "plt.imshow(dev_resized)\n",
    "plt.show()\n",
    "print(Y_dev_res[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yi3lopGhZIuT"
   },
   "source": [
    "#Baseline model\n",
    "model = tf.keras.models.Sequential([\n",
    "    InputLayer(input_shape=(48,48,1),name=\"input\"),\n",
    "    Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',name=\"conv1\"),\n",
    "    Dropout(0.25), BatchNormalization(),\n",
    "    Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',name=\"conv2\"),\n",
    "    Dropout(0.25), BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2,2),name=\"maxpool1\"),\n",
    "    Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',name=\"conv3\"),\n",
    "    Dropout(0.25), BatchNormalization(),\n",
    "    Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',name=\"conv4\"),\n",
    "    Dropout(0.25), BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(1024,input_shape=(24*24*64,1),activation='relu',name='fc1'),\n",
    "    Dense(7,input_shape=(1024,1),activation='softmax',name='fc-softmax')\n",
    "])\n",
    "\n",
    "print(\"Accuracy after training\")\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKzZuMBafv0a",
    "scrolled": true
   },
   "source": [
    "model.fit(X_train, Y_train, batch_size=32, epochs=1, validation_data=(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small_x = X_train_res[0:5000]\n",
    "small_y = Y_train_res[0:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLISdlaStbUn"
   },
   "source": [
    "history = custom_resnet.fit(\n",
    "    X_train_res,\n",
    "    Y_train_res,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_dev_res, Y_dev_res)\n",
    ")\n",
    "\n",
    "print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom_resnet.fit_generator(\n",
    "    generator           = train_generator,\n",
    "    validation_data=dev_generator, \n",
    "    steps_per_epoch=(28709) // BS,\n",
    "    shuffle=True,\n",
    "    epochs=EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "  1/224 [..............................] - ETA: 3:16:15 - loss: 2.4660 - acc: 0.132823\n",
      "  2/224 [..............................] - ETA: 2:32:37 - loss: 3.4246 - acc: 0.199224\n",
      "  3/224 [..............................] - ETA: 2:17:32 - loss: 4.3648 - acc: 0.210925\n",
      "  4/224 [..............................] - ETA: 2:09:51 - loss: 5.0890 - acc: 0.210926\n",
      "  5/224 [..............................] - ETA: 2:05:06 - loss: 5.4652 - acc: 0.201627\n",
      "  6/224 [..............................] - ETA: 2:02:18 - loss: 5.4098 - acc: 0.199228\n",
      "  7/224 [..............................] - ETA: 1:59:41 - loss: 5.0736 - acc: 0.200929\n",
      "  8/224 [>.............................] - ETA: 1:57:33 - loss: 4.7226 - acc: 0.207030\n",
      "  9/224 [>.............................] - ETA: 1:55:49 - loss: 4.4300 - acc: 0.207531\n",
      " 10/224 [>.............................] - ETA: 1:54:20 - loss: 4.1854 - acc: 0.217232\n",
      " 11/224 [>.............................] - ETA: 1:53:01 - loss: 3.9909 - acc: 0.220233\n",
      " 12/224 [>.............................] - ETA: 1:52:12 - loss: 3.8271 - acc: 0.223334\n",
      " 13/224 [>.............................] - ETA: 1:51:06 - loss: 3.6726 - acc: 0.225435\n",
      " 14/224 [>.............................] - ETA: 1:50:02 - loss: 3.5349 - acc: 0.228236\n",
      " 15/224 [=>............................] - ETA: 1:49:09 - loss: 3.4161 - acc: 0.232337\n",
      " 16/224 [=>............................] - ETA: 1:48:18 - loss: 3.3112 - acc: 0.233938\n",
      " 17/224 [=>............................] - ETA: 1:47:28 - loss: 3.2200 - acc: 0.232539\n",
      " 18/224 [=>............................] - ETA: 1:46:55 - loss: 3.1333 - acc: 0.236540\n",
      " 19/224 [=>............................] - ETA: 1:46:09 - loss: 3.0586 - acc: 0.242241\n",
      " 20/224 [=>............................] - ETA: 1:45:26 - loss: 2.9833 - acc: 0.249242\n",
      " 21/224 [=>............................] - ETA: 1:44:43 - loss: 2.9155 - acc: 0.254143\n",
      " 22/224 [=>............................] - ETA: 1:44:00 - loss: 2.8554 - acc: 0.259644\n",
      " 23/224 [==>...........................] - ETA: 1:43:20 - loss: 2.7944 - acc: 0.264945\n",
      " 24/224 [==>...........................] - ETA: 1:42:48 - loss: 2.7383 - acc: 0.271846\n",
      " 25/224 [==>...........................] - ETA: 1:42:09 - loss: 2.6892 - acc: 0.277547\n",
      " 26/224 [==>...........................] - ETA: 1:41:30 - loss: 2.6414 - acc: 0.282848\n",
      " 27/224 [==>...........................] - ETA: 1:40:52 - loss: 2.5973 - acc: 0.286749\n",
      " 28/224 [==>...........................] - ETA: 1:40:16 - loss: 2.5539 - acc: 0.293250\n",
      " 29/224 [==>...........................] - ETA: 1:39:38 - loss: 2.5068 - acc: 0.300651\n",
      " 30/224 [===>..........................] - ETA: 1:39:08 - loss: 2.4701 - acc: 0.304452\n",
      " 31/224 [===>..........................] - ETA: 1:38:32 - loss: 2.4357 - acc: 0.308753\n",
      " 32/224 [===>..........................] - ETA: 1:37:56 - loss: 2.4056 - acc: 0.314554\n",
      " 33/224 [===>..........................] - ETA: 1:37:21 - loss: 2.3697 - acc: 0.321055\n",
      " 34/224 [===>..........................] - ETA: 1:36:47 - loss: 2.3408 - acc: 0.325456\n",
      " 35/224 [===>..........................] - ETA: 1:36:12 - loss: 2.3129 - acc: 0.327957\n",
      " 36/224 [===>..........................] - ETA: 1:35:43 - loss: 2.2873 - acc: 0.331858\n",
      " 37/224 [===>..........................] - ETA: 1:35:09 - loss: 2.2606 - acc: 0.335959\n",
      " 38/224 [====>.........................] - ETA: 1:34:36 - loss: 2.2373 - acc: 0.339660\n",
      " 39/224 [====>.........................] - ETA: 1:34:02 - loss: 2.2143 - acc: 0.343561\n",
      " 40/224 [====>.........................] - ETA: 1:33:28 - loss: 2.1904 - acc: 0.347762\n",
      " 41/224 [====>.........................] - ETA: 1:32:54 - loss: 2.1680 - acc: 0.351863\n",
      " 42/224 [====>.........................] - ETA: 1:32:25 - loss: 2.1452 - acc: 0.356064\n",
      " 43/224 [====>.........................] - ETA: 1:31:52 - loss: 2.1244 - acc: 0.359465\n",
      " 44/224 [====>.........................] - ETA: 1:31:19 - loss: 2.1061 - acc: 0.361966\n",
      " 45/224 [=====>........................] - ETA: 1:30:47 - loss: 2.0884 - acc: 0.364267\n",
      " 46/224 [=====>........................] - ETA: 1:30:14 - loss: 2.0687 - acc: 0.369268\n",
      " 47/224 [=====>........................] - ETA: 1:29:41 - loss: 2.0528 - acc: 0.372569\n",
      " 48/224 [=====>........................] - ETA: 1:29:12 - loss: 2.0351 - acc: 0.375870\n",
      " 49/224 [=====>........................] - ETA: 1:28:40 - loss: 2.0185 - acc: 0.378571\n",
      " 50/224 [=====>........................] - ETA: 1:28:08 - loss: 1.9985 - acc: 0.383072\n",
      " 51/224 [=====>........................] - ETA: 1:27:36 - loss: 1.9824 - acc: 0.385973\n",
      " 52/224 [=====>........................] - ETA: 1:27:04 - loss: 1.9658 - acc: 0.390074\n",
      " 53/224 [======>.......................] - ETA: 1:26:31 - loss: 1.9526 - acc: 0.391175\n",
      " 54/224 [======>.......................] - ETA: 1:26:03 - loss: 1.9375 - acc: 0.394876\n",
      " 55/224 [======>.......................] - ETA: 1:25:31 - loss: 1.9228 - acc: 0.398477\n",
      " 56/224 [======>.......................] - ETA: 1:24:59 - loss: 1.9122 - acc: 0.399878\n",
      " 57/224 [======>.......................] - ETA: 1:24:26 - loss: 1.8971 - acc: 0.402179\n",
      " 58/224 [======>.......................] - ETA: 1:23:54 - loss: 1.8831 - acc: 0.405780\n",
      " 59/224 [======>.......................] - ETA: 1:23:22 - loss: 1.8743 - acc: 0.407381\n",
      " 60/224 [=======>......................] - ETA: 1:22:53 - loss: 1.8616 - acc: 0.409682\n",
      " 61/224 [=======>......................] - ETA: 1:22:21 - loss: 1.8490 - acc: 0.412583\n",
      " 62/224 [=======>......................] - ETA: 1:21:49 - loss: 1.8364 - acc: 0.415284\n",
      " 63/224 [=======>......................] - ETA: 1:21:17 - loss: 1.8241 - acc: 0.418385\n",
      " 64/224 [=======>......................] - ETA: 1:20:45 - loss: 1.8115 - acc: 0.421186\n",
      " 65/224 [=======>......................] - ETA: 1:20:14 - loss: 1.8001 - acc: 0.424387\n",
      " 66/224 [=======>......................] - ETA: 1:19:45 - loss: 1.7890 - acc: 0.427188\n",
      " 67/224 [=======>......................] - ETA: 1:19:13 - loss: 1.7796 - acc: 0.430089\n",
      " 68/224 [========>.....................] - ETA: 1:18:41 - loss: 1.7690 - acc: 0.432790\n",
      " 69/224 [========>.....................] - ETA: 1:18:10 - loss: 1.7610 - acc: 0.434291\n",
      " 70/224 [========>.....................] - ETA: 1:17:39 - loss: 1.7525 - acc: 0.435992\n",
      " 71/224 [========>.....................] - ETA: 1:17:07 - loss: 1.7415 - acc: 0.438793\n",
      " 72/224 [========>.....................] - ETA: 1:16:39 - loss: 1.7321 - acc: 0.441294\n",
      " 73/224 [========>.....................] - ETA: 1:16:07 - loss: 1.7216 - acc: 0.443995\n",
      " 74/224 [========>.....................] - ETA: 1:15:36 - loss: 1.7117 - acc: 0.446996\n",
      " 75/224 [=========>....................] - ETA: 1:15:05 - loss: 1.7032 - acc: 0.448997\n",
      " 76/224 [=========>....................] - ETA: 1:14:34 - loss: 1.6960 - acc: 0.450198\n",
      " 77/224 [=========>....................] - ETA: 1:14:03 - loss: 1.6879 - acc: 0.451699\n",
      " 78/224 [=========>....................] - ETA: 1:13:34 - loss: 1.6781 - acc: 0.4545100\n",
      " 79/224 [=========>....................] - ETA: 1:13:03 - loss: 1.6721 - acc: 0.4557101\n",
      " 80/224 [=========>....................] - ETA: 1:12:32 - loss: 1.6663 - acc: 0.4566102\n",
      " 81/224 [=========>....................] - ETA: 1:12:01 - loss: 1.6593 - acc: 0.4578103\n",
      " 82/224 [=========>....................] - ETA: 1:11:30 - loss: 1.6531 - acc: 0.4591104\n",
      " 83/224 [==========>...................] - ETA: 1:10:59 - loss: 1.6458 - acc: 0.4599105\n",
      " 84/224 [==========>...................] - ETA: 1:10:30 - loss: 1.6403 - acc: 0.4610106\n",
      " 85/224 [==========>...................] - ETA: 1:09:58 - loss: 1.6338 - acc: 0.4627107\n",
      " 86/224 [==========>...................] - ETA: 1:09:28 - loss: 1.6269 - acc: 0.4645108\n",
      " 87/224 [==========>...................] - ETA: 1:08:57 - loss: 1.6198 - acc: 0.4657109\n",
      " 88/224 [==========>...................] - ETA: 1:08:26 - loss: 1.6134 - acc: 0.4672110\n",
      " 89/224 [==========>...................] - ETA: 1:07:55 - loss: 1.6067 - acc: 0.4684111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90/224 [===========>..................] - ETA: 1:07:25 - loss: 1.6010 - acc: 0.4697112\n",
      " 91/224 [===========>..................] - ETA: 1:06:55 - loss: 1.5950 - acc: 0.4712113\n",
      " 92/224 [===========>..................] - ETA: 1:06:24 - loss: 1.5892 - acc: 0.4721114\n",
      " 93/224 [===========>..................] - ETA: 1:05:54 - loss: 1.5827 - acc: 0.4740115\n",
      " 94/224 [===========>..................] - ETA: 1:05:23 - loss: 1.5757 - acc: 0.4756116\n",
      " 95/224 [===========>..................] - ETA: 1:04:52 - loss: 1.5690 - acc: 0.4776117\n",
      " 96/224 [===========>..................] - ETA: 1:04:23 - loss: 1.5648 - acc: 0.4785118\n",
      " 97/224 [===========>..................] - ETA: 1:03:52 - loss: 1.5582 - acc: 0.4805119\n",
      " 98/224 [============>.................] - ETA: 1:03:21 - loss: 1.5534 - acc: 0.4818120\n",
      " 99/224 [============>.................] - ETA: 1:02:51 - loss: 1.5483 - acc: 0.4830121\n",
      "100/224 [============>.................] - ETA: 1:02:21 - loss: 1.5440 - acc: 0.4838122\n",
      "101/224 [============>.................] - ETA: 1:01:50 - loss: 1.5391 - acc: 0.4850123\n",
      "102/224 [============>.................] - ETA: 1:01:20 - loss: 1.5326 - acc: 0.4872124\n",
      "103/224 [============>.................] - ETA: 1:00:50 - loss: 1.5272 - acc: 0.4884125\n",
      "104/224 [============>.................] - ETA: 1:00:19 - loss: 1.5214 - acc: 0.4899126\n",
      "105/224 [=============>................] - ETA: 59:48 - loss: 1.5169 - acc: 0.4908  127\n",
      "106/224 [=============>................] - ETA: 59:18 - loss: 1.5113 - acc: 0.4923128\n",
      "107/224 [=============>................] - ETA: 58:47 - loss: 1.5074 - acc: 0.4926129\n",
      "108/224 [=============>................] - ETA: 58:18 - loss: 1.5015 - acc: 0.4942130\n",
      "109/224 [=============>................] - ETA: 57:47 - loss: 1.4960 - acc: 0.4958131\n",
      "110/224 [=============>................] - ETA: 57:17 - loss: 1.4929 - acc: 0.4966132\n",
      "111/224 [=============>................] - ETA: 56:46 - loss: 1.4895 - acc: 0.4975133\n",
      "112/224 [==============>...............] - ETA: 56:16 - loss: 1.4853 - acc: 0.4983134\n",
      "113/224 [==============>...............] - ETA: 55:45 - loss: 1.4809 - acc: 0.4996135\n",
      "114/224 [==============>...............] - ETA: 55:16 - loss: 1.4776 - acc: 0.5008136\n",
      "115/224 [==============>...............] - ETA: 54:45 - loss: 1.4741 - acc: 0.5012137\n",
      "116/224 [==============>...............] - ETA: 54:15 - loss: 1.4701 - acc: 0.5022138\n",
      "117/224 [==============>...............] - ETA: 53:44 - loss: 1.4661 - acc: 0.5033139\n",
      "118/224 [==============>...............] - ETA: 53:14 - loss: 1.4623 - acc: 0.5040140\n",
      "119/224 [==============>...............] - ETA: 52:43 - loss: 1.4574 - acc: 0.5056141\n",
      "120/224 [===============>..............] - ETA: 52:14 - loss: 1.4524 - acc: 0.5072142\n",
      "121/224 [===============>..............] - ETA: 51:43 - loss: 1.4481 - acc: 0.5088143\n",
      "122/224 [===============>..............] - ETA: 51:13 - loss: 1.4447 - acc: 0.5095144\n",
      "123/224 [===============>..............] - ETA: 50:42 - loss: 1.4400 - acc: 0.5101145\n",
      "124/224 [===============>..............] - ETA: 50:12 - loss: 1.4358 - acc: 0.5117146\n",
      "125/224 [===============>..............] - ETA: 49:41 - loss: 1.4328 - acc: 0.5126147\n",
      "126/224 [===============>..............] - ETA: 49:12 - loss: 1.4295 - acc: 0.5133148\n",
      "127/224 [================>.............] - ETA: 48:41 - loss: 1.4263 - acc: 0.5142149\n",
      "128/224 [================>.............] - ETA: 48:11 - loss: 1.4240 - acc: 0.5153150\n",
      "129/224 [================>.............] - ETA: 47:41 - loss: 1.4216 - acc: 0.5157151\n",
      "130/224 [================>.............] - ETA: 47:10 - loss: 1.4184 - acc: 0.5162152\n",
      "131/224 [================>.............] - ETA: 46:40 - loss: 1.4150 - acc: 0.5170153\n",
      "132/224 [================>.............] - ETA: 46:10 - loss: 1.4109 - acc: 0.5188154\n",
      "133/224 [================>.............] - ETA: 45:40 - loss: 1.4081 - acc: 0.5193155\n",
      "134/224 [================>.............] - ETA: 45:11 - loss: 1.4049 - acc: 0.5201156\n",
      "135/224 [=================>............] - ETA: 44:40 - loss: 1.4023 - acc: 0.5207157\n",
      "136/224 [=================>............] - ETA: 44:10 - loss: 1.4000 - acc: 0.5215158\n",
      "137/224 [=================>............] - ETA: 43:40 - loss: 1.3973 - acc: 0.5222159\n",
      "138/224 [=================>............] - ETA: 43:10 - loss: 1.3942 - acc: 0.5233160\n",
      "139/224 [=================>............] - ETA: 42:40 - loss: 1.3905 - acc: 0.5241161\n",
      "140/224 [=================>............] - ETA: 42:10 - loss: 1.3874 - acc: 0.5246162\n",
      "141/224 [=================>............] - ETA: 41:39 - loss: 1.3841 - acc: 0.5254163\n",
      "142/224 [==================>...........] - ETA: 41:09 - loss: 1.3823 - acc: 0.5262164\n",
      "143/224 [==================>...........] - ETA: 40:39 - loss: 1.3793 - acc: 0.5274165\n",
      "144/224 [==================>...........] - ETA: 40:09 - loss: 1.3763 - acc: 0.5280166\n",
      "145/224 [==================>...........] - ETA: 39:39 - loss: 1.3728 - acc: 0.5290167\n",
      "146/224 [==================>...........] - ETA: 39:09 - loss: 1.3694 - acc: 0.5302168\n",
      "147/224 [==================>...........] - ETA: 38:38 - loss: 1.3660 - acc: 0.5314169\n",
      "148/224 [==================>...........] - ETA: 38:08 - loss: 1.3632 - acc: 0.5320170\n",
      "149/224 [==================>...........] - ETA: 37:38 - loss: 1.3603 - acc: 0.5328171\n",
      "150/224 [===================>..........] - ETA: 37:08 - loss: 1.3571 - acc: 0.5337172\n",
      "151/224 [===================>..........] - ETA: 36:38 - loss: 1.3550 - acc: 0.5345173\n",
      "152/224 [===================>..........] - ETA: 36:08 - loss: 1.3543 - acc: 0.5347174\n",
      "153/224 [===================>..........] - ETA: 35:37 - loss: 1.3514 - acc: 0.5353175\n",
      "154/224 [===================>..........] - ETA: 35:07 - loss: 1.3480 - acc: 0.5362176\n",
      "155/224 [===================>..........] - ETA: 34:37 - loss: 1.3454 - acc: 0.5371177\n",
      "156/224 [===================>..........] - ETA: 34:07 - loss: 1.3431 - acc: 0.5377178\n",
      "157/224 [====================>.........] - ETA: 33:37 - loss: 1.3402 - acc: 0.5385179\n",
      "158/224 [====================>.........] - ETA: 33:07 - loss: 1.3370 - acc: 0.5395180\n",
      "159/224 [====================>.........] - ETA: 32:36 - loss: 1.3351 - acc: 0.5399181\n",
      "160/224 [====================>.........] - ETA: 32:06 - loss: 1.3329 - acc: 0.5404182\n",
      "161/224 [====================>.........] - ETA: 31:36 - loss: 1.3311 - acc: 0.5408183\n",
      "162/224 [====================>.........] - ETA: 31:06 - loss: 1.3284 - acc: 0.5418184\n",
      "163/224 [====================>.........] - ETA: 30:36 - loss: 1.3260 - acc: 0.5425185\n",
      "164/224 [====================>.........] - ETA: 30:06 - loss: 1.3240 - acc: 0.5430186\n",
      "165/224 [=====================>........] - ETA: 29:36 - loss: 1.3217 - acc: 0.5437187\n",
      "166/224 [=====================>........] - ETA: 29:05 - loss: 1.3193 - acc: 0.5442188\n",
      "167/224 [=====================>........] - ETA: 28:35 - loss: 1.3162 - acc: 0.5451189\n",
      "168/224 [=====================>........] - ETA: 28:06 - loss: 1.3138 - acc: 0.5456190\n",
      "169/224 [=====================>........] - ETA: 27:35 - loss: 1.3112 - acc: 0.5464191\n",
      "170/224 [=====================>........] - ETA: 27:05 - loss: 1.3096 - acc: 0.5470192\n",
      "171/224 [=====================>........] - ETA: 26:35 - loss: 1.3079 - acc: 0.5476193\n",
      "172/224 [======================>.......] - ETA: 26:05 - loss: 1.3059 - acc: 0.5481194\n",
      "173/224 [======================>.......] - ETA: 25:35 - loss: 1.3038 - acc: 0.5488195\n",
      "174/224 [======================>.......] - ETA: 25:05 - loss: 1.3022 - acc: 0.5491196\n",
      "175/224 [======================>.......] - ETA: 24:35 - loss: 1.3006 - acc: 0.5496197\n",
      "176/224 [======================>.......] - ETA: 24:05 - loss: 1.2979 - acc: 0.5505198\n",
      "177/224 [======================>.......] - ETA: 23:35 - loss: 1.2956 - acc: 0.5513199\n",
      "178/224 [======================>.......] - ETA: 23:05 - loss: 1.2936 - acc: 0.5518200\n",
      "179/224 [======================>.......] - ETA: 22:35 - loss: 1.2917 - acc: 0.5522201\n",
      "180/224 [=======================>......] - ETA: 22:05 - loss: 1.2895 - acc: 0.5530202\n",
      "181/224 [=======================>......] - ETA: 21:35 - loss: 1.2872 - acc: 0.5539203\n",
      "182/224 [=======================>......] - ETA: 21:04 - loss: 1.2855 - acc: 0.5543204\n",
      "183/224 [=======================>......] - ETA: 20:34 - loss: 1.2832 - acc: 0.5549205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/224 [=======================>......] - ETA: 20:04 - loss: 1.2807 - acc: 0.5557206\n",
      "185/224 [=======================>......] - ETA: 19:34 - loss: 1.2785 - acc: 0.5564207\n",
      "186/224 [=======================>......] - ETA: 19:04 - loss: 1.2772 - acc: 0.5569208\n",
      "187/224 [========================>.....] - ETA: 18:34 - loss: 1.2751 - acc: 0.5574209\n",
      "188/224 [========================>.....] - ETA: 18:04 - loss: 1.2735 - acc: 0.5579210\n",
      "189/224 [========================>.....] - ETA: 17:33 - loss: 1.2709 - acc: 0.5586211\n",
      "190/224 [========================>.....] - ETA: 17:03 - loss: 1.2694 - acc: 0.5591212\n",
      "191/224 [========================>.....] - ETA: 16:33 - loss: 1.2670 - acc: 0.5599213\n",
      "192/224 [========================>.....] - ETA: 16:03 - loss: 1.2649 - acc: 0.5603214\n",
      "193/224 [========================>.....] - ETA: 15:33 - loss: 1.2626 - acc: 0.5610215\n",
      "194/224 [========================>.....] - ETA: 15:03 - loss: 1.2616 - acc: 0.5613216\n",
      "195/224 [=========================>....] - ETA: 14:33 - loss: 1.2597 - acc: 0.5618217\n",
      "196/224 [=========================>....] - ETA: 14:03 - loss: 1.2580 - acc: 0.5626218\n",
      "197/224 [=========================>....] - ETA: 13:32 - loss: 1.2556 - acc: 0.5635219\n",
      "198/224 [=========================>....] - ETA: 13:03 - loss: 1.2544 - acc: 0.5636220\n",
      "199/224 [=========================>....] - ETA: 12:32 - loss: 1.2529 - acc: 0.5639221\n",
      "200/224 [=========================>....] - ETA: 12:02 - loss: 1.2515 - acc: 0.5644222\n",
      "201/224 [=========================>....] - ETA: 11:32 - loss: 1.2495 - acc: 0.5653223\n",
      "202/224 [==========================>...] - ETA: 11:02 - loss: 1.2472 - acc: 0.5660224\n",
      "203/224 [==========================>...] - ETA: 10:32 - loss: 1.2449 - acc: 0.5668225\n",
      "204/224 [==========================>...] - ETA: 10:02 - loss: 1.2432 - acc: 0.5674226\n",
      "205/224 [==========================>...] - ETA: 9:32 - loss: 1.2414 - acc: 0.5681 227\n",
      "206/224 [==========================>...] - ETA: 9:02 - loss: 1.2401 - acc: 0.5682228\n",
      "207/224 [==========================>...] - ETA: 8:31 - loss: 1.2387 - acc: 0.5685229\n",
      "208/224 [==========================>...] - ETA: 8:01 - loss: 1.2373 - acc: 0.5688230\n",
      "209/224 [==========================>...] - ETA: 7:31 - loss: 1.2351 - acc: 0.5695231\n",
      "210/224 [===========================>..] - ETA: 7:01 - loss: 1.2339 - acc: 0.5698232\n",
      "211/224 [===========================>..] - ETA: 6:31 - loss: 1.2315 - acc: 0.5706233\n",
      "212/224 [===========================>..] - ETA: 6:01 - loss: 1.2295 - acc: 0.5712234\n",
      "213/224 [===========================>..] - ETA: 5:31 - loss: 1.2280 - acc: 0.5720235\n",
      "223/224 [============================>.] - ETA: 30s - loss: 1.2143 - acc: 0.5757 236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "224/224 [==============================] - 7121s 32s/step - loss: 1.2130 - acc: 0.5761 - val_loss: 1.1054 - val_acc: 0.5943\n",
      "Epoch 2/20\n",
      "  1/224 [..............................] - ETA: 30:52 - loss: 0.9028 - acc: 0.6757"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f28780a6a935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3589\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "H = custom_resnet.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=(28709) // BS,\n",
    "    validation_data=dev_generator,\n",
    "    validation_steps=3589 // BS,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_resnet.save('transfer_learning_DG.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom_resnet.evaluate(X_train_res, Y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom_resnet.evaluate(X_dev_res, Y_dev_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "fer2013.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
