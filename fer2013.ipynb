{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fer2013.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amilkh/cs230-fer/blob/75-soa/fer2013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwdg7Sv3XBaP",
        "outputId": "e107abf2-7b1b-4efe-9551-ff1fd0579082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "%tensorflow_version 1.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.4`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2nz38mJZXN_P",
        "outputId": "20176123-bfc5-4ab8-e4a8-67cf48792b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI204HIYEdLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "b7a5b3fb-74b2-4c57-da3b-6f2dd7fec141"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqF6jK_u5Q7d",
        "colab_type": "code",
        "outputId": "ec958bea-7adb-4c6a-f440-75c0d278575a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPO33wZKzHsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 300\n",
        "BS = 128\n",
        "DROPOUT_RATE = 0.4\n",
        "SGD_LEARNING_RATE = 0.01\n",
        "SGD_DECAY = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYIE_ufI2Rf9",
        "colab_type": "text"
      },
      "source": [
        "#data = pd.read_csv('/content/drive/My Drive/cs230 project/collab/fer2013/fer2013.csv')\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/cs230 project/collab/fer2013/icml_face_data.csv') \n",
        "\n",
        "print(data.head())\n",
        "\n",
        "print(data.columns)\n",
        "\n",
        "data_train = data[data[' Usage'] == 'Training']\n",
        "\n",
        "print('Number samples in the training dataset: ', data_train.shape[0])\n",
        "data_dev = data[data[' Usage'] == 'PublicTest']\n",
        "\n",
        "print('Number samples in the development dataset: ', data_dev.shape[0]) \n",
        "print(data_dev.head())\n",
        "data_test = data[data[' Usage'] == 'PrivateTest']\n",
        "\n",
        "print('Number samples in the development dataset: ', data_dev.shape[0]) \n",
        "print(data_test.head())\n",
        "data_train.to_csv('/content/drive/My Drive/cs230 project/collab/fer2013/train.csv') \n",
        "data_dev.to_csv('/content/drive/My Drive/cs230 project/collab/fer2013/dev.csv') \n",
        "data_test.to_csv('/content/drive/My Drive/cs230 project/collab/fer2013/test.csv')\n",
        "\n",
        "print(data_train.shape) \n",
        "print(data_dev.shape) \n",
        "print(data_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hAjh6yOLYPZm",
        "colab": {}
      },
      "source": [
        "# Function that reads the data from the csv file, increases the size of the images and returns the images and their labels\n",
        "    # dataset: Data path\n",
        "def get_data(dataset):\n",
        "    \n",
        "    file_stream = file_io.FileIO(dataset, mode='r')\n",
        "    data = pd.read_csv(file_stream)\n",
        "\n",
        "    #data = pd.read_csv('fer2013/fer2013.csv')\n",
        "    data[' pixels'] = data[' pixels'].apply(lambda x: [int(pixel) for pixel in x.split()])\n",
        "\n",
        "    # Retrieve train input and target\n",
        "    X, Y = data[' pixels'].tolist(), data['emotion'].values\n",
        "    \n",
        "    # Reshape images to 4D (num_samples, width, height, num_channels)\n",
        "    X_res = np.array(X, dtype='float32').reshape(-1,48,48,1)\n",
        "    # Normalize images with max (the maximum pixel intensity is 255)\n",
        "    X_res = X_res/255.0\n",
        "    #image_resized = resize(image, (image.shape[0] // 4, image.shape[1] // 4), anti_aliasing=True)\n",
        "\n",
        "    Y_res = np.zeros((Y.size, 7))\n",
        "    Y_res[np.arange(Y.size),Y] = 1    \n",
        "    \n",
        "    return  X_res, Y_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm5tqGr-zHsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_dataset_dir = '/content/drive/My Drive/cs230 project/collab/fer2013/train.csv'\n",
        "dev_dataset_dir = '/content/drive/My Drive/cs230 project/collab/fer2013/dev.csv'\n",
        "# Data preparation\n",
        "X_train, Y_train  = get_data(training_dataset_dir)\n",
        "X_dev, Y_dev      = get_data(dev_dataset_dir)\n",
        "\n",
        "# Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely\n",
        "# rescale:          Rescaling factor (defaults to None). Multiply the data by the value provided (before applying any other transformation)\n",
        "# rotation_range:   Int. Degree range for random rotations\n",
        "# shear_range:      Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
        "# zoom_range:       Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]\n",
        "# fill_mode :       Points outside the boundaries of the input are filled according to the given mode: {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}\n",
        "# horizontal_flip:  Boolean. Randomly flip inputs horizontally\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#     rotation_range  = 10,\n",
        "#     shear_range     = 5, # 10 degrees\n",
        "#     zoom_range      = 0.1,\n",
        "#     fill_mode       = 'reflect',\n",
        "#     horizontal_flip = True)\n",
        "\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#         rotation_range=15,\n",
        "#         width_shift_range=0.2,\n",
        "#         height_shift_range=0.2,\n",
        "#         horizontal_flip=True,\n",
        "#         zoom_range=0.2)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)\n",
        "\n",
        "# Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batcfillhes indefinitely, in an infinite loop\n",
        "    # x:            Data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, \n",
        "    #               it should have value 3\n",
        "    # y:            Labels\n",
        "    # batch_size:   Int (default: 32)\n",
        "train_generator = train_datagen.flow(X_train, Y_train,  batch_size  = BS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfk02gSdzHsq",
        "colab_type": "text"
      },
      "source": [
        "# Implement below paper CPCPCPFF depth 5, 2.4m params\n",
        "# http://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Kim_Fusing_Aligned_and_CVPR_2016_paper.pdf\n",
        "# Reference: https://arxiv.org/pdf/1612.02903.pdf\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (5, 5), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(96, (5, 5), activation='relu',padding='same',name=\"conv2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(256, (5, 5), activation='relu',padding='same',name=\"conv3\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
        "model.add(Conv2D(256, (5, 5), activation='relu',padding='same',name=\"conv4\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2048, activation='relu',name='fc1'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
        "\n",
        "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
        "sgd = SGD(lr=SGD_LEARNING_RATE,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG4t-6RO-g2S",
        "colab_type": "code",
        "outputId": "917030b3-d984-4006-d302-6b5a87cdcdb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "# Implement below paper CPCPCPFF depth 5, 2.4m params\n",
        "# http://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Kim_Fusing_Aligned_and_CVPR_2016_paper.pdf\n",
        "# Reference: https://arxiv.org/pdf/1612.02903.pdf\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5, 5), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
        "#model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(32, (4, 4), activation='relu',padding='same',name=\"conv2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
        "#model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu',padding='same',name=\"conv3\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu',name='fc1'))\n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
        "\n",
        "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
        "sgd = SGD(lr=SGD_LEARNING_RATE,momentum=0.9, decay=SGD_DECAY, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "#rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)\n",
        "rlrop = ReduceLROnPlateau(monitor='val_acc',mode='max',factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ3hGX2HzHsw",
        "colab_type": "code",
        "outputId": "e71fd9f4-5c0b-4d9c-d7b2-e55e22510d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit_generator(\n",
        "    generator = train_generator,\n",
        "    validation_data=(X_dev, Y_dev), \n",
        "    steps_per_epoch=len(X_train) // BS,\n",
        "    shuffle=True,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[rlrop]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/300\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 1.8950 - acc: 0.2717 - val_loss: 1.6864 - val_acc: 0.3210\n",
            "Epoch 2/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.7027 - acc: 0.3206 - val_loss: 1.5729 - val_acc: 0.3993\n",
            "Epoch 3/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.6404 - acc: 0.3522 - val_loss: 1.5189 - val_acc: 0.4071\n",
            "Epoch 4/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.5793 - acc: 0.3808 - val_loss: 1.4989 - val_acc: 0.4255\n",
            "Epoch 5/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.5431 - acc: 0.3980 - val_loss: 1.6575 - val_acc: 0.3541\n",
            "Epoch 6/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.5046 - acc: 0.4170 - val_loss: 1.4770 - val_acc: 0.4355\n",
            "Epoch 7/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.4662 - acc: 0.4309 - val_loss: 1.9229 - val_acc: 0.3575\n",
            "Epoch 8/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.4329 - acc: 0.4463 - val_loss: 1.3452 - val_acc: 0.4884\n",
            "Epoch 9/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.3999 - acc: 0.4612 - val_loss: 1.3532 - val_acc: 0.4812\n",
            "Epoch 10/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.3701 - acc: 0.4705 - val_loss: 1.5291 - val_acc: 0.4232\n",
            "Epoch 11/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.3495 - acc: 0.4809 - val_loss: 1.6036 - val_acc: 0.4269\n",
            "Epoch 12/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.3330 - acc: 0.4873 - val_loss: 1.7551 - val_acc: 0.4107\n",
            "Epoch 13/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.3133 - acc: 0.4950 - val_loss: 1.6176 - val_acc: 0.3801\n",
            "Epoch 14/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.2913 - acc: 0.5089 - val_loss: 1.2763 - val_acc: 0.5124\n",
            "Epoch 15/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.2761 - acc: 0.5103 - val_loss: 1.2600 - val_acc: 0.5188\n",
            "Epoch 16/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.2581 - acc: 0.5200 - val_loss: 1.2153 - val_acc: 0.5383\n",
            "Epoch 17/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.2418 - acc: 0.5269 - val_loss: 1.2356 - val_acc: 0.5305\n",
            "Epoch 18/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 1.2293 - acc: 0.5327 - val_loss: 1.2018 - val_acc: 0.5380\n",
            "Epoch 19/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 1.2111 - acc: 0.5399 - val_loss: 1.1927 - val_acc: 0.5508\n",
            "Epoch 20/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.2006 - acc: 0.5437 - val_loss: 1.1865 - val_acc: 0.5506\n",
            "Epoch 21/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 1.1874 - acc: 0.5521 - val_loss: 1.1744 - val_acc: 0.5570\n",
            "Epoch 22/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1796 - acc: 0.5555 - val_loss: 1.1672 - val_acc: 0.5656\n",
            "Epoch 23/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.1637 - acc: 0.5587 - val_loss: 1.1559 - val_acc: 0.5695\n",
            "Epoch 24/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1573 - acc: 0.5648 - val_loss: 1.1732 - val_acc: 0.5617\n",
            "Epoch 25/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1553 - acc: 0.5617 - val_loss: 1.2066 - val_acc: 0.5497\n",
            "Epoch 26/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1293 - acc: 0.5738 - val_loss: 1.2148 - val_acc: 0.5475\n",
            "Epoch 27/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.1258 - acc: 0.5741 - val_loss: 1.1609 - val_acc: 0.5612\n",
            "Epoch 28/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 1.1219 - acc: 0.5732 - val_loss: 1.2106 - val_acc: 0.5561\n",
            "Epoch 29/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1053 - acc: 0.5850 - val_loss: 1.1398 - val_acc: 0.5701\n",
            "Epoch 30/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1017 - acc: 0.5850 - val_loss: 1.1499 - val_acc: 0.5695\n",
            "Epoch 31/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.0917 - acc: 0.5877 - val_loss: 1.1138 - val_acc: 0.5754\n",
            "Epoch 32/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.0789 - acc: 0.5934 - val_loss: 1.1087 - val_acc: 0.5868\n",
            "Epoch 33/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.0762 - acc: 0.5959 - val_loss: 1.1556 - val_acc: 0.5662\n",
            "Epoch 34/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.0661 - acc: 0.5960 - val_loss: 1.1535 - val_acc: 0.5667\n",
            "Epoch 35/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.0626 - acc: 0.6022 - val_loss: 1.1353 - val_acc: 0.5809\n",
            "Epoch 36/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 1.0624 - acc: 0.5979 - val_loss: 1.1137 - val_acc: 0.5840\n",
            "Epoch 37/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.0512 - acc: 0.6025 - val_loss: 1.1329 - val_acc: 0.5851\n",
            "Epoch 38/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.0417 - acc: 0.6055 - val_loss: 1.1136 - val_acc: 0.5840\n",
            "Epoch 39/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.0460 - acc: 0.6067 - val_loss: 1.2135 - val_acc: 0.5522\n",
            "Epoch 40/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 1.0294 - acc: 0.6143 - val_loss: 1.1222 - val_acc: 0.5915\n",
            "Epoch 41/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.0223 - acc: 0.6121 - val_loss: 1.1590 - val_acc: 0.5782\n",
            "Epoch 42/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 1.0100 - acc: 0.6215 - val_loss: 1.1556 - val_acc: 0.5795\n",
            "Epoch 43/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 1.0151 - acc: 0.6204 - val_loss: 1.1328 - val_acc: 0.5821\n",
            "Epoch 44/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.0049 - acc: 0.6236 - val_loss: 1.1392 - val_acc: 0.5904\n",
            "Epoch 45/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.0031 - acc: 0.6228 - val_loss: 1.0935 - val_acc: 0.5965\n",
            "Epoch 46/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.9965 - acc: 0.6256 - val_loss: 1.0881 - val_acc: 0.6055\n",
            "Epoch 47/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.9913 - acc: 0.6294 - val_loss: 1.0931 - val_acc: 0.6007\n",
            "Epoch 48/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9869 - acc: 0.6291 - val_loss: 1.0746 - val_acc: 0.6066\n",
            "Epoch 49/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.9830 - acc: 0.6315 - val_loss: 1.0665 - val_acc: 0.6094\n",
            "Epoch 50/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.9701 - acc: 0.6369 - val_loss: 1.1594 - val_acc: 0.5698\n",
            "Epoch 51/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.9723 - acc: 0.6342 - val_loss: 1.1112 - val_acc: 0.6007\n",
            "Epoch 52/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.9658 - acc: 0.6381 - val_loss: 1.0882 - val_acc: 0.6041\n",
            "Epoch 53/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9619 - acc: 0.6377 - val_loss: 1.0588 - val_acc: 0.6144\n",
            "Epoch 54/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9660 - acc: 0.6381 - val_loss: 1.0767 - val_acc: 0.6007\n",
            "Epoch 55/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9445 - acc: 0.6479 - val_loss: 1.1574 - val_acc: 0.5787\n",
            "Epoch 56/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.9522 - acc: 0.6436 - val_loss: 1.1112 - val_acc: 0.5963\n",
            "Epoch 57/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.9390 - acc: 0.6456 - val_loss: 1.1748 - val_acc: 0.5837\n",
            "Epoch 58/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9445 - acc: 0.6442 - val_loss: 1.0980 - val_acc: 0.5940\n",
            "Epoch 59/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.9340 - acc: 0.6500 - val_loss: 1.0576 - val_acc: 0.6183\n",
            "Epoch 60/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.9338 - acc: 0.6518 - val_loss: 1.0671 - val_acc: 0.6166\n",
            "Epoch 61/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.9209 - acc: 0.6524 - val_loss: 1.1506 - val_acc: 0.5826\n",
            "Epoch 62/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.9270 - acc: 0.6509 - val_loss: 1.0997 - val_acc: 0.6007\n",
            "Epoch 63/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9256 - acc: 0.6537 - val_loss: 1.0911 - val_acc: 0.6147\n",
            "Epoch 64/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.9187 - acc: 0.6547 - val_loss: 1.0687 - val_acc: 0.6130\n",
            "Epoch 65/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9099 - acc: 0.6610 - val_loss: 1.0847 - val_acc: 0.6183\n",
            "Epoch 66/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9005 - acc: 0.6643 - val_loss: 1.0962 - val_acc: 0.6080\n",
            "Epoch 67/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.9085 - acc: 0.6592 - val_loss: 1.0539 - val_acc: 0.6197\n",
            "Epoch 68/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8998 - acc: 0.6647 - val_loss: 1.0629 - val_acc: 0.6124\n",
            "Epoch 69/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8953 - acc: 0.6611 - val_loss: 1.0880 - val_acc: 0.6130\n",
            "Epoch 70/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8921 - acc: 0.6703 - val_loss: 1.0632 - val_acc: 0.6180\n",
            "Epoch 71/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8853 - acc: 0.6713 - val_loss: 1.0962 - val_acc: 0.6080\n",
            "Epoch 72/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8886 - acc: 0.6666 - val_loss: 1.1039 - val_acc: 0.6038\n",
            "Epoch 73/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8817 - acc: 0.6706 - val_loss: 1.0702 - val_acc: 0.6264\n",
            "Epoch 74/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8850 - acc: 0.6681 - val_loss: 1.1067 - val_acc: 0.6016\n",
            "Epoch 75/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8748 - acc: 0.6761 - val_loss: 1.0630 - val_acc: 0.6252\n",
            "Epoch 76/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8652 - acc: 0.6770 - val_loss: 1.0601 - val_acc: 0.6213\n",
            "Epoch 77/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8650 - acc: 0.6790 - val_loss: 1.0593 - val_acc: 0.6230\n",
            "Epoch 78/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8577 - acc: 0.6777 - val_loss: 1.0567 - val_acc: 0.6186\n",
            "Epoch 79/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8596 - acc: 0.6815 - val_loss: 1.1107 - val_acc: 0.6135\n",
            "Epoch 80/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8649 - acc: 0.6775 - val_loss: 1.0655 - val_acc: 0.6219\n",
            "Epoch 81/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8527 - acc: 0.6798 - val_loss: 1.0503 - val_acc: 0.6291\n",
            "Epoch 82/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8496 - acc: 0.6820 - val_loss: 1.0680 - val_acc: 0.6255\n",
            "Epoch 83/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8412 - acc: 0.6826 - val_loss: 1.0945 - val_acc: 0.6116\n",
            "Epoch 84/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8461 - acc: 0.6852 - val_loss: 1.1903 - val_acc: 0.6043\n",
            "Epoch 85/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8439 - acc: 0.6876 - val_loss: 1.0642 - val_acc: 0.6269\n",
            "Epoch 86/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8338 - acc: 0.6902 - val_loss: 1.0865 - val_acc: 0.6147\n",
            "Epoch 87/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8261 - acc: 0.6932 - val_loss: 1.0530 - val_acc: 0.6283\n",
            "Epoch 88/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8249 - acc: 0.6903 - val_loss: 1.0922 - val_acc: 0.6264\n",
            "Epoch 89/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8269 - acc: 0.6933 - val_loss: 1.0727 - val_acc: 0.6202\n",
            "Epoch 90/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8211 - acc: 0.6940 - val_loss: 1.0733 - val_acc: 0.6197\n",
            "Epoch 91/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8253 - acc: 0.6957 - val_loss: 1.1330 - val_acc: 0.6035\n",
            "\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "Epoch 92/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.7988 - acc: 0.7018 - val_loss: 1.0537 - val_acc: 0.6353\n",
            "Epoch 93/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7990 - acc: 0.7046 - val_loss: 1.0488 - val_acc: 0.6317\n",
            "Epoch 94/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7883 - acc: 0.7068 - val_loss: 1.0503 - val_acc: 0.6317\n",
            "Epoch 95/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7945 - acc: 0.7048 - val_loss: 1.0418 - val_acc: 0.6280\n",
            "Epoch 96/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7862 - acc: 0.7092 - val_loss: 1.0620 - val_acc: 0.6375\n",
            "Epoch 97/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7839 - acc: 0.7093 - val_loss: 1.0694 - val_acc: 0.6344\n",
            "Epoch 98/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7809 - acc: 0.7099 - val_loss: 1.0522 - val_acc: 0.6414\n",
            "Epoch 99/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.7770 - acc: 0.7102 - val_loss: 1.0942 - val_acc: 0.6208\n",
            "Epoch 100/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7798 - acc: 0.7114 - val_loss: 1.0341 - val_acc: 0.6420\n",
            "Epoch 101/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7802 - acc: 0.7114 - val_loss: 1.0760 - val_acc: 0.6194\n",
            "Epoch 102/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7752 - acc: 0.7131 - val_loss: 1.0340 - val_acc: 0.6403\n",
            "Epoch 103/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7745 - acc: 0.7117 - val_loss: 1.0502 - val_acc: 0.6330\n",
            "Epoch 104/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7720 - acc: 0.7129 - val_loss: 1.0671 - val_acc: 0.6325\n",
            "Epoch 105/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7779 - acc: 0.7142 - val_loss: 1.0470 - val_acc: 0.6336\n",
            "Epoch 106/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7695 - acc: 0.7161 - val_loss: 1.0457 - val_acc: 0.6367\n",
            "Epoch 107/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7719 - acc: 0.7149 - val_loss: 1.0391 - val_acc: 0.6378\n",
            "Epoch 108/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7598 - acc: 0.7160 - val_loss: 1.0503 - val_acc: 0.6403\n",
            "Epoch 109/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7717 - acc: 0.7155 - val_loss: 1.0662 - val_acc: 0.6372\n",
            "Epoch 110/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7559 - acc: 0.7183 - val_loss: 1.0447 - val_acc: 0.6431\n",
            "Epoch 111/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7586 - acc: 0.7190 - val_loss: 1.0473 - val_acc: 0.6417\n",
            "Epoch 112/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7537 - acc: 0.7223 - val_loss: 1.0610 - val_acc: 0.6436\n",
            "Epoch 113/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.7570 - acc: 0.7202 - val_loss: 1.0547 - val_acc: 0.6403\n",
            "Epoch 114/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.7511 - acc: 0.7245 - val_loss: 1.0460 - val_acc: 0.6342\n",
            "Epoch 115/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.7485 - acc: 0.7272 - val_loss: 1.0329 - val_acc: 0.6473\n",
            "Epoch 116/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7477 - acc: 0.7251 - val_loss: 1.0558 - val_acc: 0.6431\n",
            "Epoch 117/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7408 - acc: 0.7281 - val_loss: 1.0491 - val_acc: 0.6414\n",
            "Epoch 118/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7409 - acc: 0.7236 - val_loss: 1.0705 - val_acc: 0.6269\n",
            "Epoch 119/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7440 - acc: 0.7220 - val_loss: 1.0627 - val_acc: 0.6319\n",
            "Epoch 120/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7387 - acc: 0.7288 - val_loss: 1.0571 - val_acc: 0.6314\n",
            "Epoch 121/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7364 - acc: 0.7290 - val_loss: 1.0502 - val_acc: 0.6486\n",
            "Epoch 122/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7386 - acc: 0.7255 - val_loss: 1.0711 - val_acc: 0.6353\n",
            "Epoch 123/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7271 - acc: 0.7320 - val_loss: 1.0494 - val_acc: 0.6336\n",
            "Epoch 124/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7362 - acc: 0.7293 - val_loss: 1.0624 - val_acc: 0.6420\n",
            "Epoch 125/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7273 - acc: 0.7298 - val_loss: 1.0456 - val_acc: 0.6434\n",
            "Epoch 126/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7325 - acc: 0.7282 - val_loss: 1.0592 - val_acc: 0.6397\n",
            "Epoch 127/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7224 - acc: 0.7337 - val_loss: 1.0587 - val_acc: 0.6361\n",
            "Epoch 128/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.7348 - acc: 0.7288 - val_loss: 1.0439 - val_acc: 0.6442\n",
            "Epoch 129/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7215 - acc: 0.7340 - val_loss: 1.1108 - val_acc: 0.6283\n",
            "Epoch 130/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7271 - acc: 0.7304 - val_loss: 1.0555 - val_acc: 0.6400\n",
            "Epoch 131/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7351 - acc: 0.7274 - val_loss: 1.0621 - val_acc: 0.6422\n",
            "\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "Epoch 132/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7168 - acc: 0.7351 - val_loss: 1.0457 - val_acc: 0.6442\n",
            "Epoch 133/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7199 - acc: 0.7313 - val_loss: 1.0518 - val_acc: 0.6445\n",
            "Epoch 134/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7118 - acc: 0.7378 - val_loss: 1.0583 - val_acc: 0.6456\n",
            "Epoch 135/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7143 - acc: 0.7358 - val_loss: 1.0499 - val_acc: 0.6395\n",
            "Epoch 136/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7099 - acc: 0.7369 - val_loss: 1.0414 - val_acc: 0.6392\n",
            "Epoch 137/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7104 - acc: 0.7376 - val_loss: 1.0643 - val_acc: 0.6445\n",
            "Epoch 138/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7053 - acc: 0.7383 - val_loss: 1.0740 - val_acc: 0.6317\n",
            "Epoch 139/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7059 - acc: 0.7405 - val_loss: 1.0563 - val_acc: 0.6439\n",
            "Epoch 140/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7106 - acc: 0.7354 - val_loss: 1.0475 - val_acc: 0.6439\n",
            "Epoch 141/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7011 - acc: 0.7399 - val_loss: 1.0664 - val_acc: 0.6456\n",
            "\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "Epoch 142/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.7024 - acc: 0.7399 - val_loss: 1.0474 - val_acc: 0.6453\n",
            "Epoch 143/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6989 - acc: 0.7404 - val_loss: 1.0482 - val_acc: 0.6492\n",
            "Epoch 144/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6930 - acc: 0.7473 - val_loss: 1.0512 - val_acc: 0.6475\n",
            "Epoch 145/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7040 - acc: 0.7405 - val_loss: 1.0563 - val_acc: 0.6378\n",
            "Epoch 146/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6933 - acc: 0.7446 - val_loss: 1.0547 - val_acc: 0.6378\n",
            "Epoch 147/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6956 - acc: 0.7415 - val_loss: 1.0529 - val_acc: 0.6422\n",
            "Epoch 148/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6930 - acc: 0.7430 - val_loss: 1.0515 - val_acc: 0.6442\n",
            "Epoch 149/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6988 - acc: 0.7409 - val_loss: 1.0542 - val_acc: 0.6436\n",
            "Epoch 150/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.7028 - acc: 0.7396 - val_loss: 1.0472 - val_acc: 0.6520\n",
            "Epoch 151/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6949 - acc: 0.7428 - val_loss: 1.0560 - val_acc: 0.6495\n",
            "Epoch 152/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6892 - acc: 0.7454 - val_loss: 1.0729 - val_acc: 0.6459\n",
            "Epoch 153/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6956 - acc: 0.7452 - val_loss: 1.0543 - val_acc: 0.6464\n",
            "Epoch 154/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6981 - acc: 0.7389 - val_loss: 1.0591 - val_acc: 0.6475\n",
            "Epoch 155/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6974 - acc: 0.7438 - val_loss: 1.0513 - val_acc: 0.6492\n",
            "Epoch 156/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6931 - acc: 0.7432 - val_loss: 1.0508 - val_acc: 0.6467\n",
            "Epoch 157/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6992 - acc: 0.7431 - val_loss: 1.0492 - val_acc: 0.6495\n",
            "Epoch 158/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6858 - acc: 0.7458 - val_loss: 1.0510 - val_acc: 0.6495\n",
            "Epoch 159/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6991 - acc: 0.7413 - val_loss: 1.0609 - val_acc: 0.6478\n",
            "Epoch 160/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6947 - acc: 0.7459 - val_loss: 1.0453 - val_acc: 0.6498\n",
            "\n",
            "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "Epoch 161/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6885 - acc: 0.7442 - val_loss: 1.0463 - val_acc: 0.6512\n",
            "Epoch 162/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6892 - acc: 0.7435 - val_loss: 1.0494 - val_acc: 0.6467\n",
            "Epoch 163/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.7002 - acc: 0.7436 - val_loss: 1.0476 - val_acc: 0.6475\n",
            "Epoch 164/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6830 - acc: 0.7486 - val_loss: 1.0468 - val_acc: 0.6489\n",
            "Epoch 165/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6949 - acc: 0.7438 - val_loss: 1.0555 - val_acc: 0.6450\n",
            "Epoch 166/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6867 - acc: 0.7490 - val_loss: 1.0529 - val_acc: 0.6484\n",
            "Epoch 167/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6813 - acc: 0.7493 - val_loss: 1.0522 - val_acc: 0.6470\n",
            "Epoch 168/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6918 - acc: 0.7454 - val_loss: 1.0610 - val_acc: 0.6500\n",
            "Epoch 169/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6886 - acc: 0.7449 - val_loss: 1.0510 - val_acc: 0.6528\n",
            "Epoch 170/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6890 - acc: 0.7472 - val_loss: 1.0477 - val_acc: 0.6453\n",
            "Epoch 171/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6852 - acc: 0.7475 - val_loss: 1.0453 - val_acc: 0.6506\n",
            "Epoch 172/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6807 - acc: 0.7498 - val_loss: 1.0483 - val_acc: 0.6475\n",
            "Epoch 173/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6955 - acc: 0.7442 - val_loss: 1.0503 - val_acc: 0.6495\n",
            "Epoch 174/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6917 - acc: 0.7457 - val_loss: 1.0511 - val_acc: 0.6495\n",
            "Epoch 175/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6912 - acc: 0.7431 - val_loss: 1.0588 - val_acc: 0.6467\n",
            "Epoch 176/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6838 - acc: 0.7515 - val_loss: 1.0536 - val_acc: 0.6464\n",
            "Epoch 177/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6907 - acc: 0.7464 - val_loss: 1.0493 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6851 - acc: 0.7472 - val_loss: 1.0484 - val_acc: 0.6467\n",
            "Epoch 179/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6884 - acc: 0.7462 - val_loss: 1.0499 - val_acc: 0.6442\n",
            "\n",
            "Epoch 00179: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "Epoch 180/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6801 - acc: 0.7480 - val_loss: 1.0512 - val_acc: 0.6492\n",
            "Epoch 181/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6840 - acc: 0.7480 - val_loss: 1.0489 - val_acc: 0.6484\n",
            "Epoch 182/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6833 - acc: 0.7463 - val_loss: 1.0543 - val_acc: 0.6467\n",
            "Epoch 183/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6819 - acc: 0.7494 - val_loss: 1.0520 - val_acc: 0.6484\n",
            "Epoch 184/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6871 - acc: 0.7480 - val_loss: 1.0518 - val_acc: 0.6486\n",
            "Epoch 185/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6839 - acc: 0.7485 - val_loss: 1.0541 - val_acc: 0.6473\n",
            "Epoch 186/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6818 - acc: 0.7492 - val_loss: 1.0478 - val_acc: 0.6495\n",
            "Epoch 187/300\n",
            "224/224 [==============================] - 10s 45ms/step - loss: 0.6774 - acc: 0.7487 - val_loss: 1.0508 - val_acc: 0.6512\n",
            "Epoch 188/300\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 0.6861 - acc: 0.7466 - val_loss: 1.0546 - val_acc: 0.6495\n",
            "Epoch 189/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6764 - acc: 0.7520 - val_loss: 1.0566 - val_acc: 0.6478\n",
            "\n",
            "Epoch 00189: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "Epoch 190/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6883 - acc: 0.7470 - val_loss: 1.0533 - val_acc: 0.6475\n",
            "Epoch 191/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6804 - acc: 0.7491 - val_loss: 1.0543 - val_acc: 0.6486\n",
            "Epoch 192/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6712 - acc: 0.7532 - val_loss: 1.0517 - val_acc: 0.6486\n",
            "Epoch 193/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6832 - acc: 0.7471 - val_loss: 1.0547 - val_acc: 0.6467\n",
            "Epoch 194/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6875 - acc: 0.7467 - val_loss: 1.0539 - val_acc: 0.6498\n",
            "Epoch 195/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6810 - acc: 0.7491 - val_loss: 1.0521 - val_acc: 0.6489\n",
            "Epoch 196/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6831 - acc: 0.7489 - val_loss: 1.0535 - val_acc: 0.6478\n",
            "Epoch 197/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6831 - acc: 0.7466 - val_loss: 1.0507 - val_acc: 0.6489\n",
            "Epoch 198/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6855 - acc: 0.7509 - val_loss: 1.0531 - val_acc: 0.6473\n",
            "Epoch 199/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6797 - acc: 0.7474 - val_loss: 1.0534 - val_acc: 0.6475\n",
            "\n",
            "Epoch 00199: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
            "Epoch 200/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6863 - acc: 0.7461 - val_loss: 1.0530 - val_acc: 0.6467\n",
            "Epoch 201/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6858 - acc: 0.7457 - val_loss: 1.0520 - val_acc: 0.6486\n",
            "Epoch 202/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6850 - acc: 0.7473 - val_loss: 1.0516 - val_acc: 0.6478\n",
            "Epoch 203/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6773 - acc: 0.7514 - val_loss: 1.0530 - val_acc: 0.6484\n",
            "Epoch 204/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6753 - acc: 0.7503 - val_loss: 1.0520 - val_acc: 0.6478\n",
            "Epoch 205/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6845 - acc: 0.7475 - val_loss: 1.0520 - val_acc: 0.6478\n",
            "Epoch 206/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6780 - acc: 0.7507 - val_loss: 1.0539 - val_acc: 0.6467\n",
            "Epoch 207/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6928 - acc: 0.7463 - val_loss: 1.0517 - val_acc: 0.6484\n",
            "Epoch 208/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6744 - acc: 0.7532 - val_loss: 1.0531 - val_acc: 0.6470\n",
            "Epoch 209/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6834 - acc: 0.7467 - val_loss: 1.0525 - val_acc: 0.6492\n",
            "\n",
            "Epoch 00209: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
            "Epoch 210/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6871 - acc: 0.7427 - val_loss: 1.0517 - val_acc: 0.6478\n",
            "Epoch 211/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6761 - acc: 0.7510 - val_loss: 1.0524 - val_acc: 0.6475\n",
            "Epoch 212/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6829 - acc: 0.7474 - val_loss: 1.0526 - val_acc: 0.6481\n",
            "Epoch 213/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6826 - acc: 0.7467 - val_loss: 1.0522 - val_acc: 0.6489\n",
            "Epoch 214/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6852 - acc: 0.7465 - val_loss: 1.0528 - val_acc: 0.6473\n",
            "Epoch 215/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6793 - acc: 0.7497 - val_loss: 1.0524 - val_acc: 0.6495\n",
            "Epoch 216/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6815 - acc: 0.7481 - val_loss: 1.0527 - val_acc: 0.6489\n",
            "Epoch 217/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6752 - acc: 0.7522 - val_loss: 1.0531 - val_acc: 0.6473\n",
            "Epoch 218/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6824 - acc: 0.7487 - val_loss: 1.0529 - val_acc: 0.6489\n",
            "Epoch 219/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6828 - acc: 0.7483 - val_loss: 1.0520 - val_acc: 0.6486\n",
            "\n",
            "Epoch 00219: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
            "Epoch 220/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6830 - acc: 0.7486 - val_loss: 1.0526 - val_acc: 0.6489\n",
            "Epoch 221/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6805 - acc: 0.7473 - val_loss: 1.0521 - val_acc: 0.6492\n",
            "Epoch 222/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6824 - acc: 0.7483 - val_loss: 1.0523 - val_acc: 0.6486\n",
            "Epoch 223/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6821 - acc: 0.7514 - val_loss: 1.0522 - val_acc: 0.6486\n",
            "Epoch 224/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6842 - acc: 0.7478 - val_loss: 1.0529 - val_acc: 0.6486\n",
            "Epoch 225/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6820 - acc: 0.7483 - val_loss: 1.0522 - val_acc: 0.6486\n",
            "Epoch 226/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6786 - acc: 0.7505 - val_loss: 1.0530 - val_acc: 0.6489\n",
            "Epoch 227/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6888 - acc: 0.7454 - val_loss: 1.0530 - val_acc: 0.6486\n",
            "Epoch 228/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6827 - acc: 0.7462 - val_loss: 1.0530 - val_acc: 0.6475\n",
            "Epoch 229/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6776 - acc: 0.7494 - val_loss: 1.0526 - val_acc: 0.6486\n",
            "\n",
            "Epoch 00229: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 230/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.6861 - acc: 0.7462 - val_loss: 1.0531 - val_acc: 0.6486\n",
            "Epoch 231/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6795 - acc: 0.7499 - val_loss: 1.0529 - val_acc: 0.6489\n",
            "Epoch 232/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6854 - acc: 0.7475 - val_loss: 1.0525 - val_acc: 0.6486\n",
            "Epoch 233/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6825 - acc: 0.7484 - val_loss: 1.0529 - val_acc: 0.6481\n",
            "Epoch 234/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6823 - acc: 0.7490 - val_loss: 1.0525 - val_acc: 0.6481\n",
            "Epoch 235/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6775 - acc: 0.7509 - val_loss: 1.0529 - val_acc: 0.6486\n",
            "Epoch 236/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6841 - acc: 0.7445 - val_loss: 1.0524 - val_acc: 0.6484\n",
            "Epoch 237/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6762 - acc: 0.7493 - val_loss: 1.0526 - val_acc: 0.6489\n",
            "Epoch 238/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6889 - acc: 0.7450 - val_loss: 1.0527 - val_acc: 0.6489\n",
            "Epoch 239/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6844 - acc: 0.7483 - val_loss: 1.0524 - val_acc: 0.6489\n",
            "Epoch 240/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6883 - acc: 0.7468 - val_loss: 1.0526 - val_acc: 0.6489\n",
            "Epoch 241/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6767 - acc: 0.7495 - val_loss: 1.0527 - val_acc: 0.6492\n",
            "Epoch 242/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6867 - acc: 0.7467 - val_loss: 1.0526 - val_acc: 0.6484\n",
            "Epoch 243/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6787 - acc: 0.7495 - val_loss: 1.0526 - val_acc: 0.6484\n",
            "Epoch 244/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6850 - acc: 0.7467 - val_loss: 1.0529 - val_acc: 0.6475\n",
            "Epoch 245/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6779 - acc: 0.7493 - val_loss: 1.0530 - val_acc: 0.6492\n",
            "Epoch 246/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6853 - acc: 0.7486 - val_loss: 1.0528 - val_acc: 0.6478\n",
            "Epoch 247/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6790 - acc: 0.7510 - val_loss: 1.0531 - val_acc: 0.6481\n",
            "Epoch 248/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6932 - acc: 0.7430 - val_loss: 1.0522 - val_acc: 0.6481\n",
            "Epoch 249/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6867 - acc: 0.7481 - val_loss: 1.0523 - val_acc: 0.6481\n",
            "Epoch 250/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6759 - acc: 0.7532 - val_loss: 1.0525 - val_acc: 0.6484\n",
            "Epoch 251/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6810 - acc: 0.7482 - val_loss: 1.0521 - val_acc: 0.6489\n",
            "Epoch 252/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6833 - acc: 0.7480 - val_loss: 1.0532 - val_acc: 0.6492\n",
            "Epoch 253/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6783 - acc: 0.7501 - val_loss: 1.0528 - val_acc: 0.6484\n",
            "Epoch 254/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6804 - acc: 0.7509 - val_loss: 1.0535 - val_acc: 0.6486\n",
            "Epoch 255/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6802 - acc: 0.7497 - val_loss: 1.0527 - val_acc: 0.6492\n",
            "Epoch 256/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6850 - acc: 0.7469 - val_loss: 1.0533 - val_acc: 0.6478\n",
            "Epoch 257/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6824 - acc: 0.7493 - val_loss: 1.0526 - val_acc: 0.6481\n",
            "Epoch 258/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6872 - acc: 0.7466 - val_loss: 1.0531 - val_acc: 0.6486\n",
            "Epoch 259/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6877 - acc: 0.7461 - val_loss: 1.0527 - val_acc: 0.6492\n",
            "Epoch 260/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6794 - acc: 0.7470 - val_loss: 1.0524 - val_acc: 0.6486\n",
            "Epoch 261/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6772 - acc: 0.7509 - val_loss: 1.0525 - val_acc: 0.6486\n",
            "Epoch 262/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6876 - acc: 0.7492 - val_loss: 1.0529 - val_acc: 0.6486\n",
            "Epoch 263/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6809 - acc: 0.7512 - val_loss: 1.0529 - val_acc: 0.6489\n",
            "Epoch 264/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6868 - acc: 0.7484 - val_loss: 1.0528 - val_acc: 0.6481\n",
            "Epoch 265/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6796 - acc: 0.7481 - val_loss: 1.0528 - val_acc: 0.6486\n",
            "Epoch 266/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6814 - acc: 0.7496 - val_loss: 1.0529 - val_acc: 0.6492\n",
            "Epoch 267/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6822 - acc: 0.7473 - val_loss: 1.0533 - val_acc: 0.6492\n",
            "Epoch 268/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6780 - acc: 0.7510 - val_loss: 1.0534 - val_acc: 0.6481\n",
            "Epoch 269/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6832 - acc: 0.7472 - val_loss: 1.0528 - val_acc: 0.6486\n",
            "Epoch 270/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6806 - acc: 0.7488 - val_loss: 1.0526 - val_acc: 0.6484\n",
            "Epoch 271/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6816 - acc: 0.7467 - val_loss: 1.0527 - val_acc: 0.6481\n",
            "Epoch 272/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6766 - acc: 0.7496 - val_loss: 1.0523 - val_acc: 0.6484\n",
            "Epoch 273/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6838 - acc: 0.7477 - val_loss: 1.0527 - val_acc: 0.6489\n",
            "Epoch 274/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6774 - acc: 0.7484 - val_loss: 1.0526 - val_acc: 0.6484\n",
            "Epoch 275/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6795 - acc: 0.7515 - val_loss: 1.0526 - val_acc: 0.6486\n",
            "Epoch 276/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6830 - acc: 0.7500 - val_loss: 1.0527 - val_acc: 0.6478\n",
            "Epoch 277/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6808 - acc: 0.7497 - val_loss: 1.0529 - val_acc: 0.6484\n",
            "Epoch 278/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6732 - acc: 0.7524 - val_loss: 1.0527 - val_acc: 0.6475\n",
            "Epoch 279/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6884 - acc: 0.7459 - val_loss: 1.0528 - val_acc: 0.6486\n",
            "Epoch 280/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6819 - acc: 0.7498 - val_loss: 1.0528 - val_acc: 0.6478\n",
            "Epoch 281/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6791 - acc: 0.7498 - val_loss: 1.0530 - val_acc: 0.6492\n",
            "Epoch 282/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6841 - acc: 0.7475 - val_loss: 1.0523 - val_acc: 0.6486\n",
            "Epoch 283/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6886 - acc: 0.7442 - val_loss: 1.0534 - val_acc: 0.6489\n",
            "Epoch 284/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6813 - acc: 0.7477 - val_loss: 1.0522 - val_acc: 0.6489\n",
            "Epoch 285/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6830 - acc: 0.7464 - val_loss: 1.0521 - val_acc: 0.6484\n",
            "Epoch 286/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6837 - acc: 0.7484 - val_loss: 1.0530 - val_acc: 0.6484\n",
            "Epoch 287/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6815 - acc: 0.7505 - val_loss: 1.0533 - val_acc: 0.6495\n",
            "Epoch 288/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6822 - acc: 0.7491 - val_loss: 1.0524 - val_acc: 0.6489\n",
            "Epoch 289/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.6808 - acc: 0.7486 - val_loss: 1.0525 - val_acc: 0.6492\n",
            "Epoch 290/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6841 - acc: 0.7458 - val_loss: 1.0526 - val_acc: 0.6484\n",
            "Epoch 291/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6812 - acc: 0.7485 - val_loss: 1.0532 - val_acc: 0.6489\n",
            "Epoch 292/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6816 - acc: 0.7486 - val_loss: 1.0524 - val_acc: 0.6484\n",
            "Epoch 293/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6792 - acc: 0.7503 - val_loss: 1.0527 - val_acc: 0.6492\n",
            "Epoch 294/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6813 - acc: 0.7496 - val_loss: 1.0527 - val_acc: 0.6486\n",
            "Epoch 295/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6809 - acc: 0.7510 - val_loss: 1.0522 - val_acc: 0.6489\n",
            "Epoch 296/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6726 - acc: 0.7524 - val_loss: 1.0526 - val_acc: 0.6486\n",
            "Epoch 297/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.6780 - acc: 0.7463 - val_loss: 1.0524 - val_acc: 0.6486\n",
            "Epoch 298/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.6833 - acc: 0.7489 - val_loss: 1.0525 - val_acc: 0.6489\n",
            "Epoch 299/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6829 - acc: 0.7475 - val_loss: 1.0524 - val_acc: 0.6484\n",
            "Epoch 300/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.6794 - acc: 0.7505 - val_loss: 1.0526 - val_acc: 0.6489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-FSBxRPXHSZE",
        "outputId": "391ceeff-3b2c-43b0-d3a7-d43118d1e392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print('\\n# Evaluate on dev data')\n",
        "results_dev = model.evaluate(X_dev, Y_dev , batch_size=128)\n",
        "print('dev loss, dev acc:', results_dev)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on dev data\n",
            "3589/3589 [==============================] - 0s 70us/step\n",
            "dev loss, dev acc: [1.052624678904049, 0.648927277801561]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b9-RdV1zHs_",
        "colab_type": "code",
        "outputId": "2995d739-d46a-4e1a-ea68-ed059954e32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "test_dataset_dir = '/content/drive/My Drive/cs230 project/collab/fer2013/test.csv'\n",
        "X_test, Y_test      = get_data(test_dataset_dir)\n",
        "\n",
        "print('\\n# Evaluate on test data')\n",
        "results_test = model.evaluate(X_test, Y_test , batch_size=128)\n",
        "print('test loss, test acc:', results_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "3589/3589 [==============================] - 0s 74us/step\n",
            "test loss, test acc: [1.0130413163169973, 0.648927277801561]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr6u8rO4zHtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "b8d3b91c-65b4-43da-ec12-a81b854f6947"
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc', 'lr'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSqkop4lAKaWi3P8HhYvhK5t6x9EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pQfysJQzHtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_str = '-SGD_LR_%.5f' % SGD_LEARNING_RATE\n",
        "epoch_str = '-EPOCHS_' + str(EPOCHS)\n",
        "bs_str = '-BS_' + str(BS)\n",
        "dropout_str = '-DROPOUT_' + str(DROPOUT_RATE)\n",
        "test_acc = 'test_acc_%.3f' % results_test[1]\n",
        "model.save('/content/drive/My Drive/cs230 project/models/soa' + lr_str + epoch_str + bs_str + dropout_str + test_acc + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMAJ9smqKdD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# configure image data augmentation\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "# make a prediction using test-time augmentation\n",
        "def tta_prediction(datagen, model, image, n_examples):\n",
        "\t# convert image into dataset\n",
        "\tsamples = np.expand_dims(image, 0)\n",
        "\t# prepare iterator\n",
        "\tit = datagen.flow(samples, batch_size=n_examples)\n",
        "\t# make predictions for each augmented image\n",
        "\tyhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
        "\t# sum across predictions\n",
        "\tsummed = np.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\treturn np.argmax(summed)\n",
        " \n",
        " # evaluate a model on a dataset using test-time augmentation\n",
        "def tta_evaluate_model(model, testX, testY):\n",
        "\t# configure image data augmentation\n",
        "\tdatagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\t# define the number of augmented images to generate per test set image\n",
        "\tn_examples_per_image = 7\n",
        "\tyhats = list()\n",
        "\tfor i in range(len(testX)):\n",
        "\t\t# make augmented prediction\n",
        "\t\tyhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\n",
        "\t\t# store for evaluation\n",
        "\t\tyhats.append(yhat)\n",
        "\t# calculate accuracy\n",
        "\ttestY_labels = np.argmax(testY, axis=1)\n",
        "\tacc = accuracy_score(testY_labels, yhats)\n",
        "\treturn acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfTihcArMdUk",
        "colab_type": "code",
        "outputId": "97f640ea-f418-43c3-fe59-e04adb6ac725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print('\\n# Evaluate on test data')\n",
        "TTA_results_test = tta_evaluate_model(model, X_test, Y_test)\n",
        "print('test loss, test acc:', results_test)\n",
        "print('TTA test acc:', TTA_results_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "test loss, test acc: [1.0130413163169973, 0.648927277801561]\n",
            "TTA test acc: 0.6623014767344664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V57Y-EomzHtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}