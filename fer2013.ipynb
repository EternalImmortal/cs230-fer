{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fer2013.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwdg7Sv3XBaP",
        "outputId": "fc2292fd-2a20-41f0-87c6-18eac6f80003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "%tensorflow_version 1.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.4`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2nz38mJZXN_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "138ac2c8-6b36-4b65-929d-a81ab3b0a714"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqF6jK_u5Q7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6705d959-56d2-4bce-9988-7f7ed1a889d0"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPO33wZKzHsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 300\n",
        "BS = 128\n",
        "DROPOUT_RATE = 0.4\n",
        "SGD_LEARNING_RATE = 0.01\n",
        "SGD_DECAY = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYIE_ufI2Rf9",
        "colab_type": "text"
      },
      "source": [
        "#data = pd.read_csv('/content/drive/My Drive/cs230 project/collab/fer2013/fer2013.csv')\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/cs230 project/collab/fer2013/icml_face_data.csv') \n",
        "\n",
        "print(data.head())\n",
        "\n",
        "print(data.columns)\n",
        "\n",
        "data_train = data[data[' Usage'] == 'Training']\n",
        "\n",
        "print('Number samples in the training dataset: ', data_train.shape[0])\n",
        "data_dev = data[data[' Usage'] == 'PublicTest']\n",
        "\n",
        "print('Number samples in the development dataset: ', data_dev.shape[0]) \n",
        "print(data_dev.head())\n",
        "data_test = data[data[' Usage'] == 'PrivateTest']\n",
        "\n",
        "print('Number samples in the development dataset: ', data_dev.shape[0]) \n",
        "print(data_test.head())\n",
        "data_train.to_csv('/content/drive/My Drive/cs230 project/collab/fer2013/train.csv') \n",
        "data_dev.to_csv('/content/drive/My Drive/cs230 project/collab/fer2013/dev.csv') \n",
        "data_test.to_csv('/content/drive/My Drive/cs230 project/collab/fer2013/test.csv')\n",
        "\n",
        "print(data_train.shape) \n",
        "print(data_dev.shape) \n",
        "print(data_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hAjh6yOLYPZm",
        "colab": {}
      },
      "source": [
        "# Function that reads the data from the csv file, increases the size of the images and returns the images and their labels\n",
        "    # dataset: Data path\n",
        "def get_data(dataset):\n",
        "    \n",
        "    file_stream = file_io.FileIO(dataset, mode='r')\n",
        "    data = pd.read_csv(file_stream)\n",
        "\n",
        "    #data = pd.read_csv('fer2013/fer2013.csv')\n",
        "    data[' pixels'] = data[' pixels'].apply(lambda x: [int(pixel) for pixel in x.split()])\n",
        "\n",
        "    # Retrieve train input and target\n",
        "    X, Y = data[' pixels'].tolist(), data['emotion'].values\n",
        "    \n",
        "    # Reshape images to 4D (num_samples, width, height, num_channels)\n",
        "    X_res = np.array(X, dtype='float32').reshape(-1,48,48,1)\n",
        "    # Normalize images with max (the maximum pixel intensity is 255)\n",
        "    X_res = X_res/255.0\n",
        "    #image_resized = resize(image, (image.shape[0] // 4, image.shape[1] // 4), anti_aliasing=True)\n",
        "\n",
        "    Y_res = np.zeros((Y.size, 7))\n",
        "    Y_res[np.arange(Y.size),Y] = 1    \n",
        "    \n",
        "    return  X_res, Y_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm5tqGr-zHsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_dataset_dir = '/content/drive/My Drive/cs230 project/collab/fer2013/train.csv'\n",
        "dev_dataset_dir = '/content/drive/My Drive/cs230 project/collab/fer2013/dev.csv'\n",
        "# Data preparation\n",
        "X_train, Y_train  = get_data(training_dataset_dir)\n",
        "X_dev, Y_dev      = get_data(dev_dataset_dir)\n",
        "\n",
        "# Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely\n",
        "# rescale:          Rescaling factor (defaults to None). Multiply the data by the value provided (before applying any other transformation)\n",
        "# rotation_range:   Int. Degree range for random rotations\n",
        "# shear_range:      Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
        "# zoom_range:       Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]\n",
        "# fill_mode :       Points outside the boundaries of the input are filled according to the given mode: {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}\n",
        "# horizontal_flip:  Boolean. Randomly flip inputs horizontally\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#     rotation_range  = 10,\n",
        "#     shear_range     = 5, # 10 degrees\n",
        "#     zoom_range      = 0.1,\n",
        "#     fill_mode       = 'reflect',\n",
        "#     horizontal_flip = True)\n",
        "\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#         rotation_range=15,\n",
        "#         width_shift_range=0.2,\n",
        "#         height_shift_range=0.2,\n",
        "#         horizontal_flip=True,\n",
        "#         zoom_range=0.2)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)\n",
        "\n",
        "# Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batcfillhes indefinitely, in an infinite loop\n",
        "    # x:            Data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, \n",
        "    #               it should have value 3\n",
        "    # y:            Labels\n",
        "    # batch_size:   Int (default: 32)\n",
        "train_generator = train_datagen.flow(X_train, Y_train,  batch_size  = BS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfk02gSdzHsq",
        "colab_type": "text"
      },
      "source": [
        "# Implement below paper CPCPCPFF depth 5, 2.4m params\n",
        "# http://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Kim_Fusing_Aligned_and_CVPR_2016_paper.pdf\n",
        "# Reference: https://arxiv.org/pdf/1612.02903.pdf\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (5, 5), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(96, (5, 5), activation='relu',padding='same',name=\"conv2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(256, (5, 5), activation='relu',padding='same',name=\"conv3\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
        "model.add(Conv2D(256, (5, 5), activation='relu',padding='same',name=\"conv4\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2048, activation='relu',name='fc1'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
        "\n",
        "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
        "sgd = SGD(lr=SGD_LEARNING_RATE,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG4t-6RO-g2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "586559b5-5637-4d56-8762-3072549a88ff"
      },
      "source": [
        "# Implement below paper CPCPCPFF depth 5, 2.4m params\n",
        "# http://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Kim_Fusing_Aligned_and_CVPR_2016_paper.pdf\n",
        "# Reference: https://arxiv.org/pdf/1612.02903.pdf\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5, 5), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
        "#model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(32, (4, 4), activation='relu',padding='same',name=\"conv2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
        "#model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu',padding='same',name=\"conv3\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu',name='fc1'))\n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
        "\n",
        "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
        "sgd = SGD(lr=SGD_LEARNING_RATE,momentum=0.9, decay=SGD_DECAY, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "#rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)\n",
        "rlrop = ReduceLROnPlateau(monitor='val_acc',mode='max',factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ3hGX2HzHsw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b47bf37-1b81-4c5b-af34-2424a0a1eb6c"
      },
      "source": [
        "history = model.fit_generator(\n",
        "    generator = train_generator,\n",
        "    validation_data=(X_dev, Y_dev), \n",
        "    steps_per_epoch=len(X_train) // BS,\n",
        "    shuffle=True,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[rlrop]) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/300\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.8975 - acc: 0.2676 - val_loss: 1.6910 - val_acc: 0.3299\n",
            "Epoch 2/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.7086 - acc: 0.3133 - val_loss: 1.6057 - val_acc: 0.3764\n",
            "Epoch 3/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.6439 - acc: 0.3448 - val_loss: 1.5577 - val_acc: 0.3817\n",
            "Epoch 4/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.5868 - acc: 0.3799 - val_loss: 1.4573 - val_acc: 0.4372\n",
            "Epoch 5/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.5392 - acc: 0.3964 - val_loss: 1.6041 - val_acc: 0.3753\n",
            "Epoch 6/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.4976 - acc: 0.4155 - val_loss: 1.4164 - val_acc: 0.4544\n",
            "Epoch 7/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.4522 - acc: 0.4380 - val_loss: 1.6669 - val_acc: 0.3575\n",
            "Epoch 8/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.4168 - acc: 0.4538 - val_loss: 1.3622 - val_acc: 0.4787\n",
            "Epoch 9/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.3841 - acc: 0.4652 - val_loss: 1.4186 - val_acc: 0.4338\n",
            "Epoch 10/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.3509 - acc: 0.4831 - val_loss: 1.5994 - val_acc: 0.3644\n",
            "Epoch 11/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.3188 - acc: 0.4921 - val_loss: 1.3320 - val_acc: 0.4815\n",
            "Epoch 12/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.2984 - acc: 0.5024 - val_loss: 1.3120 - val_acc: 0.5032\n",
            "Epoch 13/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.2760 - acc: 0.5111 - val_loss: 1.3232 - val_acc: 0.4859\n",
            "Epoch 14/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.2510 - acc: 0.5241 - val_loss: 1.2202 - val_acc: 0.5352\n",
            "Epoch 15/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.2433 - acc: 0.5246 - val_loss: 1.1851 - val_acc: 0.5483\n",
            "Epoch 16/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.2213 - acc: 0.5365 - val_loss: 1.2465 - val_acc: 0.5107\n",
            "Epoch 17/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.2129 - acc: 0.5386 - val_loss: 1.2746 - val_acc: 0.5155\n",
            "Epoch 18/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.1929 - acc: 0.5464 - val_loss: 1.2200 - val_acc: 0.5325\n",
            "Epoch 19/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 1.1780 - acc: 0.5496 - val_loss: 1.2828 - val_acc: 0.5060\n",
            "Epoch 20/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.1723 - acc: 0.5552 - val_loss: 1.2040 - val_acc: 0.5366\n",
            "Epoch 21/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1605 - acc: 0.5599 - val_loss: 1.2859 - val_acc: 0.5130\n",
            "Epoch 22/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 1.1495 - acc: 0.5634 - val_loss: 1.1590 - val_acc: 0.5612\n",
            "Epoch 23/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1319 - acc: 0.5748 - val_loss: 1.1720 - val_acc: 0.5506\n",
            "Epoch 24/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1272 - acc: 0.5732 - val_loss: 1.1616 - val_acc: 0.5667\n",
            "Epoch 25/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 1.1161 - acc: 0.5788 - val_loss: 1.1241 - val_acc: 0.5726\n",
            "Epoch 26/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 1.1101 - acc: 0.5802 - val_loss: 1.1427 - val_acc: 0.5631\n",
            "Epoch 27/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 1.0966 - acc: 0.5862 - val_loss: 1.1645 - val_acc: 0.5642\n",
            "Epoch 28/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 1.0950 - acc: 0.5818 - val_loss: 1.1433 - val_acc: 0.5765\n",
            "Epoch 29/300\n",
            "224/224 [==============================] - 12s 54ms/step - loss: 1.0728 - acc: 0.5943 - val_loss: 1.1651 - val_acc: 0.5620\n",
            "Epoch 30/300\n",
            "224/224 [==============================] - 12s 54ms/step - loss: 1.0799 - acc: 0.5901 - val_loss: 1.0836 - val_acc: 0.5957\n",
            "Epoch 31/300\n",
            "224/224 [==============================] - 12s 54ms/step - loss: 1.0570 - acc: 0.5981 - val_loss: 1.1827 - val_acc: 0.5595\n",
            "Epoch 32/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 1.0623 - acc: 0.5973 - val_loss: 1.1041 - val_acc: 0.5812\n",
            "Epoch 33/300\n",
            "224/224 [==============================] - 12s 51ms/step - loss: 1.0503 - acc: 0.6035 - val_loss: 1.1407 - val_acc: 0.5804\n",
            "Epoch 34/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 1.0466 - acc: 0.6061 - val_loss: 1.1225 - val_acc: 0.5812\n",
            "Epoch 35/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 1.0358 - acc: 0.6099 - val_loss: 1.0846 - val_acc: 0.5918\n",
            "Epoch 36/300\n",
            "224/224 [==============================] - 12s 51ms/step - loss: 1.0246 - acc: 0.6138 - val_loss: 1.1123 - val_acc: 0.5837\n",
            "Epoch 37/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 1.0313 - acc: 0.6121 - val_loss: 1.1570 - val_acc: 0.5787\n",
            "Epoch 38/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 1.0121 - acc: 0.6202 - val_loss: 1.1213 - val_acc: 0.5879\n",
            "Epoch 39/300\n",
            "224/224 [==============================] - 12s 53ms/step - loss: 1.0132 - acc: 0.6201 - val_loss: 1.1981 - val_acc: 0.5556\n",
            "Epoch 40/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 1.0021 - acc: 0.6216 - val_loss: 1.0896 - val_acc: 0.5954\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "Epoch 41/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 0.9833 - acc: 0.6301 - val_loss: 1.0483 - val_acc: 0.6135\n",
            "Epoch 42/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 0.9780 - acc: 0.6301 - val_loss: 1.0730 - val_acc: 0.6049\n",
            "Epoch 43/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 0.9690 - acc: 0.6372 - val_loss: 1.0672 - val_acc: 0.6071\n",
            "Epoch 44/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 0.9597 - acc: 0.6403 - val_loss: 1.0662 - val_acc: 0.6094\n",
            "Epoch 45/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 0.9580 - acc: 0.6414 - val_loss: 1.0655 - val_acc: 0.6133\n",
            "Epoch 46/300\n",
            "224/224 [==============================] - 12s 55ms/step - loss: 0.9554 - acc: 0.6402 - val_loss: 1.0686 - val_acc: 0.6130\n",
            "Epoch 47/300\n",
            "224/224 [==============================] - 12s 53ms/step - loss: 0.9529 - acc: 0.6420 - val_loss: 1.0870 - val_acc: 0.6071\n",
            "Epoch 48/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.9507 - acc: 0.6430 - val_loss: 1.0860 - val_acc: 0.6043\n",
            "Epoch 49/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.9481 - acc: 0.6496 - val_loss: 1.0707 - val_acc: 0.6110\n",
            "Epoch 50/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.9371 - acc: 0.6489 - val_loss: 1.0495 - val_acc: 0.6127\n",
            "Epoch 51/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.9321 - acc: 0.6505 - val_loss: 1.0483 - val_acc: 0.6108\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "Epoch 52/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.9326 - acc: 0.6512 - val_loss: 1.0389 - val_acc: 0.6160\n",
            "Epoch 53/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.9207 - acc: 0.6539 - val_loss: 1.0354 - val_acc: 0.6199\n",
            "Epoch 54/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.9204 - acc: 0.6554 - val_loss: 1.0539 - val_acc: 0.6138\n",
            "Epoch 55/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.9207 - acc: 0.6511 - val_loss: 1.0402 - val_acc: 0.6211\n",
            "Epoch 56/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.9131 - acc: 0.6600 - val_loss: 1.0646 - val_acc: 0.6113\n",
            "Epoch 57/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.9178 - acc: 0.6571 - val_loss: 1.0447 - val_acc: 0.6155\n",
            "Epoch 58/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.9154 - acc: 0.6572 - val_loss: 1.0339 - val_acc: 0.6252\n",
            "Epoch 59/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.9162 - acc: 0.6559 - val_loss: 1.0354 - val_acc: 0.6233\n",
            "Epoch 60/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.9076 - acc: 0.6598 - val_loss: 1.0409 - val_acc: 0.6230\n",
            "Epoch 61/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.9008 - acc: 0.6642 - val_loss: 1.0605 - val_acc: 0.6110\n",
            "Epoch 62/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.9077 - acc: 0.6586 - val_loss: 1.0544 - val_acc: 0.6213\n",
            "Epoch 63/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.9011 - acc: 0.6626 - val_loss: 1.0643 - val_acc: 0.6188\n",
            "Epoch 64/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.9030 - acc: 0.6618 - val_loss: 1.0397 - val_acc: 0.6275\n",
            "Epoch 65/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.9109 - acc: 0.6586 - val_loss: 1.0340 - val_acc: 0.6289\n",
            "Epoch 66/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8955 - acc: 0.6663 - val_loss: 1.0493 - val_acc: 0.6208\n",
            "Epoch 67/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8955 - acc: 0.6659 - val_loss: 1.0664 - val_acc: 0.6160\n",
            "Epoch 68/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8927 - acc: 0.6651 - val_loss: 1.0579 - val_acc: 0.6152\n",
            "Epoch 69/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8981 - acc: 0.6627 - val_loss: 1.0509 - val_acc: 0.6152\n",
            "Epoch 70/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8927 - acc: 0.6643 - val_loss: 1.0251 - val_acc: 0.6233\n",
            "Epoch 71/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8995 - acc: 0.6652 - val_loss: 1.0278 - val_acc: 0.6230\n",
            "Epoch 72/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8933 - acc: 0.6638 - val_loss: 1.0743 - val_acc: 0.6133\n",
            "Epoch 73/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8898 - acc: 0.6689 - val_loss: 1.0331 - val_acc: 0.6233\n",
            "Epoch 74/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8935 - acc: 0.6644 - val_loss: 1.0217 - val_acc: 0.6286\n",
            "Epoch 75/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8890 - acc: 0.6706 - val_loss: 1.0254 - val_acc: 0.6266\n",
            "\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "Epoch 76/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8879 - acc: 0.6648 - val_loss: 1.0301 - val_acc: 0.6275\n",
            "Epoch 77/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8862 - acc: 0.6672 - val_loss: 1.0263 - val_acc: 0.6294\n",
            "Epoch 78/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8793 - acc: 0.6716 - val_loss: 1.0289 - val_acc: 0.6252\n",
            "Epoch 79/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8737 - acc: 0.6752 - val_loss: 1.0292 - val_acc: 0.6230\n",
            "Epoch 80/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8813 - acc: 0.6676 - val_loss: 1.0277 - val_acc: 0.6303\n",
            "Epoch 81/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8704 - acc: 0.6770 - val_loss: 1.0454 - val_acc: 0.6225\n",
            "Epoch 82/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8775 - acc: 0.6733 - val_loss: 1.0275 - val_acc: 0.6269\n",
            "Epoch 83/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8797 - acc: 0.6714 - val_loss: 1.0249 - val_acc: 0.6289\n",
            "Epoch 84/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8666 - acc: 0.6773 - val_loss: 1.0367 - val_acc: 0.6264\n",
            "Epoch 85/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8728 - acc: 0.6732 - val_loss: 1.0342 - val_acc: 0.6264\n",
            "Epoch 86/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8764 - acc: 0.6746 - val_loss: 1.0358 - val_acc: 0.6244\n",
            "Epoch 87/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8743 - acc: 0.6706 - val_loss: 1.0427 - val_acc: 0.6258\n",
            "Epoch 88/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8621 - acc: 0.6796 - val_loss: 1.0253 - val_acc: 0.6308\n",
            "Epoch 89/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8727 - acc: 0.6747 - val_loss: 1.0295 - val_acc: 0.6319\n",
            "Epoch 90/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8745 - acc: 0.6757 - val_loss: 1.0357 - val_acc: 0.6241\n",
            "Epoch 91/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8683 - acc: 0.6734 - val_loss: 1.0275 - val_acc: 0.6236\n",
            "Epoch 92/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8716 - acc: 0.6717 - val_loss: 1.0245 - val_acc: 0.6275\n",
            "Epoch 93/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8678 - acc: 0.6760 - val_loss: 1.0259 - val_acc: 0.6300\n",
            "Epoch 94/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8749 - acc: 0.6732 - val_loss: 1.0212 - val_acc: 0.6314\n",
            "Epoch 95/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8563 - acc: 0.6815 - val_loss: 1.0522 - val_acc: 0.6244\n",
            "Epoch 96/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8669 - acc: 0.6802 - val_loss: 1.0239 - val_acc: 0.6303\n",
            "Epoch 97/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8666 - acc: 0.6772 - val_loss: 1.0552 - val_acc: 0.6205\n",
            "Epoch 98/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8628 - acc: 0.6770 - val_loss: 1.0426 - val_acc: 0.6250\n",
            "Epoch 99/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8591 - acc: 0.6771 - val_loss: 1.0268 - val_acc: 0.6278\n",
            "\n",
            "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "Epoch 100/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8646 - acc: 0.6777 - val_loss: 1.0387 - val_acc: 0.6280\n",
            "Epoch 101/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8599 - acc: 0.6767 - val_loss: 1.0258 - val_acc: 0.6280\n",
            "Epoch 102/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8576 - acc: 0.6788 - val_loss: 1.0284 - val_acc: 0.6272\n",
            "Epoch 103/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8679 - acc: 0.6778 - val_loss: 1.0258 - val_acc: 0.6289\n",
            "Epoch 104/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8616 - acc: 0.6791 - val_loss: 1.0255 - val_acc: 0.6305\n",
            "Epoch 105/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8615 - acc: 0.6767 - val_loss: 1.0359 - val_acc: 0.6291\n",
            "Epoch 106/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8626 - acc: 0.6772 - val_loss: 1.0300 - val_acc: 0.6286\n",
            "Epoch 107/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8624 - acc: 0.6795 - val_loss: 1.0298 - val_acc: 0.6291\n",
            "Epoch 108/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8567 - acc: 0.6813 - val_loss: 1.0204 - val_acc: 0.6305\n",
            "Epoch 109/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8463 - acc: 0.6834 - val_loss: 1.0348 - val_acc: 0.6280\n",
            "\n",
            "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "Epoch 110/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8633 - acc: 0.6774 - val_loss: 1.0307 - val_acc: 0.6278\n",
            "Epoch 111/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8516 - acc: 0.6840 - val_loss: 1.0280 - val_acc: 0.6261\n",
            "Epoch 112/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8584 - acc: 0.6777 - val_loss: 1.0243 - val_acc: 0.6266\n",
            "Epoch 113/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8568 - acc: 0.6800 - val_loss: 1.0276 - val_acc: 0.6294\n",
            "Epoch 114/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8647 - acc: 0.6766 - val_loss: 1.0238 - val_acc: 0.6286\n",
            "Epoch 115/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8578 - acc: 0.6784 - val_loss: 1.0263 - val_acc: 0.6272\n",
            "Epoch 116/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8587 - acc: 0.6786 - val_loss: 1.0264 - val_acc: 0.6280\n",
            "Epoch 117/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8511 - acc: 0.6824 - val_loss: 1.0224 - val_acc: 0.6305\n",
            "Epoch 118/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8664 - acc: 0.6767 - val_loss: 1.0279 - val_acc: 0.6294\n",
            "Epoch 119/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8519 - acc: 0.6836 - val_loss: 1.0281 - val_acc: 0.6283\n",
            "\n",
            "Epoch 00119: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "Epoch 120/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8477 - acc: 0.6823 - val_loss: 1.0229 - val_acc: 0.6303\n",
            "Epoch 121/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8599 - acc: 0.6798 - val_loss: 1.0258 - val_acc: 0.6294\n",
            "Epoch 122/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8439 - acc: 0.6846 - val_loss: 1.0263 - val_acc: 0.6314\n",
            "Epoch 123/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8593 - acc: 0.6797 - val_loss: 1.0277 - val_acc: 0.6291\n",
            "Epoch 124/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8537 - acc: 0.6831 - val_loss: 1.0224 - val_acc: 0.6311\n",
            "Epoch 125/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8555 - acc: 0.6820 - val_loss: 1.0252 - val_acc: 0.6297\n",
            "Epoch 126/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8552 - acc: 0.6784 - val_loss: 1.0266 - val_acc: 0.6294\n",
            "Epoch 127/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8507 - acc: 0.6821 - val_loss: 1.0242 - val_acc: 0.6297\n",
            "Epoch 128/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8596 - acc: 0.6798 - val_loss: 1.0239 - val_acc: 0.6297\n",
            "Epoch 129/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8577 - acc: 0.6778 - val_loss: 1.0247 - val_acc: 0.6305\n",
            "\n",
            "Epoch 00129: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
            "Epoch 130/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8524 - acc: 0.6805 - val_loss: 1.0251 - val_acc: 0.6297\n",
            "Epoch 131/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8537 - acc: 0.6839 - val_loss: 1.0249 - val_acc: 0.6294\n",
            "Epoch 132/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8495 - acc: 0.6798 - val_loss: 1.0255 - val_acc: 0.6289\n",
            "Epoch 133/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8568 - acc: 0.6809 - val_loss: 1.0278 - val_acc: 0.6289\n",
            "Epoch 134/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8558 - acc: 0.6789 - val_loss: 1.0255 - val_acc: 0.6311\n",
            "Epoch 135/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8508 - acc: 0.6840 - val_loss: 1.0262 - val_acc: 0.6308\n",
            "Epoch 136/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8526 - acc: 0.6795 - val_loss: 1.0257 - val_acc: 0.6289\n",
            "Epoch 137/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8584 - acc: 0.6779 - val_loss: 1.0254 - val_acc: 0.6305\n",
            "Epoch 138/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8497 - acc: 0.6835 - val_loss: 1.0243 - val_acc: 0.6328\n",
            "Epoch 139/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8555 - acc: 0.6826 - val_loss: 1.0246 - val_acc: 0.6303\n",
            "Epoch 140/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8491 - acc: 0.6821 - val_loss: 1.0263 - val_acc: 0.6280\n",
            "Epoch 141/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8529 - acc: 0.6805 - val_loss: 1.0252 - val_acc: 0.6308\n",
            "Epoch 142/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8555 - acc: 0.6817 - val_loss: 1.0238 - val_acc: 0.6291\n",
            "Epoch 143/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8511 - acc: 0.6836 - val_loss: 1.0243 - val_acc: 0.6308\n",
            "Epoch 144/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8541 - acc: 0.6817 - val_loss: 1.0253 - val_acc: 0.6291\n",
            "Epoch 145/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8549 - acc: 0.6810 - val_loss: 1.0236 - val_acc: 0.6319\n",
            "Epoch 146/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8472 - acc: 0.6854 - val_loss: 1.0249 - val_acc: 0.6294\n",
            "Epoch 147/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8549 - acc: 0.6818 - val_loss: 1.0264 - val_acc: 0.6303\n",
            "Epoch 148/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8486 - acc: 0.6825 - val_loss: 1.0243 - val_acc: 0.6303\n",
            "\n",
            "Epoch 00148: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
            "Epoch 149/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8563 - acc: 0.6810 - val_loss: 1.0251 - val_acc: 0.6303\n",
            "Epoch 150/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8511 - acc: 0.6805 - val_loss: 1.0254 - val_acc: 0.6294\n",
            "Epoch 151/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8516 - acc: 0.6811 - val_loss: 1.0247 - val_acc: 0.6297\n",
            "Epoch 152/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8435 - acc: 0.6870 - val_loss: 1.0258 - val_acc: 0.6291\n",
            "Epoch 153/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8604 - acc: 0.6792 - val_loss: 1.0253 - val_acc: 0.6300\n",
            "Epoch 154/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8488 - acc: 0.6859 - val_loss: 1.0243 - val_acc: 0.6297\n",
            "Epoch 155/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8578 - acc: 0.6820 - val_loss: 1.0246 - val_acc: 0.6300\n",
            "Epoch 156/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8494 - acc: 0.6832 - val_loss: 1.0252 - val_acc: 0.6303\n",
            "Epoch 157/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8427 - acc: 0.6848 - val_loss: 1.0243 - val_acc: 0.6297\n",
            "Epoch 158/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8568 - acc: 0.6812 - val_loss: 1.0245 - val_acc: 0.6308\n",
            "\n",
            "Epoch 00158: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
            "Epoch 159/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8453 - acc: 0.6844 - val_loss: 1.0242 - val_acc: 0.6305\n",
            "Epoch 160/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8553 - acc: 0.6848 - val_loss: 1.0236 - val_acc: 0.6314\n",
            "Epoch 161/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8544 - acc: 0.6815 - val_loss: 1.0245 - val_acc: 0.6300\n",
            "Epoch 162/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8486 - acc: 0.6838 - val_loss: 1.0248 - val_acc: 0.6300\n",
            "Epoch 163/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8512 - acc: 0.6802 - val_loss: 1.0246 - val_acc: 0.6305\n",
            "Epoch 164/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8603 - acc: 0.6817 - val_loss: 1.0246 - val_acc: 0.6303\n",
            "Epoch 165/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8511 - acc: 0.6838 - val_loss: 1.0242 - val_acc: 0.6294\n",
            "Epoch 166/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8609 - acc: 0.6772 - val_loss: 1.0243 - val_acc: 0.6294\n",
            "Epoch 167/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8578 - acc: 0.6800 - val_loss: 1.0249 - val_acc: 0.6303\n",
            "Epoch 168/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8462 - acc: 0.6863 - val_loss: 1.0247 - val_acc: 0.6294\n",
            "\n",
            "Epoch 00168: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 169/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8516 - acc: 0.6822 - val_loss: 1.0251 - val_acc: 0.6291\n",
            "Epoch 170/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8515 - acc: 0.6821 - val_loss: 1.0244 - val_acc: 0.6297\n",
            "Epoch 171/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8614 - acc: 0.6805 - val_loss: 1.0252 - val_acc: 0.6297\n",
            "Epoch 172/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8485 - acc: 0.6855 - val_loss: 1.0250 - val_acc: 0.6300\n",
            "Epoch 173/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8563 - acc: 0.6807 - val_loss: 1.0243 - val_acc: 0.6303\n",
            "Epoch 174/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8495 - acc: 0.6815 - val_loss: 1.0248 - val_acc: 0.6303\n",
            "Epoch 175/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8531 - acc: 0.6837 - val_loss: 1.0244 - val_acc: 0.6300\n",
            "Epoch 176/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8573 - acc: 0.6823 - val_loss: 1.0249 - val_acc: 0.6305\n",
            "Epoch 177/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8537 - acc: 0.6797 - val_loss: 1.0246 - val_acc: 0.6305\n",
            "Epoch 178/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8537 - acc: 0.6809 - val_loss: 1.0244 - val_acc: 0.6300\n",
            "Epoch 179/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8532 - acc: 0.6828 - val_loss: 1.0247 - val_acc: 0.6300\n",
            "Epoch 180/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8520 - acc: 0.6834 - val_loss: 1.0248 - val_acc: 0.6305\n",
            "Epoch 181/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8616 - acc: 0.6771 - val_loss: 1.0246 - val_acc: 0.6294\n",
            "Epoch 182/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8536 - acc: 0.6841 - val_loss: 1.0241 - val_acc: 0.6305\n",
            "Epoch 183/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8552 - acc: 0.6814 - val_loss: 1.0240 - val_acc: 0.6300\n",
            "Epoch 184/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8599 - acc: 0.6781 - val_loss: 1.0247 - val_acc: 0.6300\n",
            "Epoch 185/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8487 - acc: 0.6843 - val_loss: 1.0247 - val_acc: 0.6297\n",
            "Epoch 186/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8527 - acc: 0.6825 - val_loss: 1.0255 - val_acc: 0.6300\n",
            "Epoch 187/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8594 - acc: 0.6791 - val_loss: 1.0243 - val_acc: 0.6297\n",
            "Epoch 188/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8540 - acc: 0.6817 - val_loss: 1.0240 - val_acc: 0.6294\n",
            "Epoch 189/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8533 - acc: 0.6822 - val_loss: 1.0249 - val_acc: 0.6294\n",
            "Epoch 190/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8512 - acc: 0.6843 - val_loss: 1.0242 - val_acc: 0.6291\n",
            "Epoch 191/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8448 - acc: 0.6840 - val_loss: 1.0244 - val_acc: 0.6297\n",
            "Epoch 192/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8608 - acc: 0.6781 - val_loss: 1.0245 - val_acc: 0.6294\n",
            "Epoch 193/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8497 - acc: 0.6863 - val_loss: 1.0247 - val_acc: 0.6300\n",
            "Epoch 194/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8518 - acc: 0.6793 - val_loss: 1.0249 - val_acc: 0.6300\n",
            "Epoch 195/300\n",
            "224/224 [==============================] - 10s 46ms/step - loss: 0.8558 - acc: 0.6802 - val_loss: 1.0245 - val_acc: 0.6308\n",
            "Epoch 196/300\n",
            "224/224 [==============================] - 11s 47ms/step - loss: 0.8482 - acc: 0.6825 - val_loss: 1.0243 - val_acc: 0.6300\n",
            "Epoch 197/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8538 - acc: 0.6801 - val_loss: 1.0250 - val_acc: 0.6300\n",
            "Epoch 198/300\n",
            "224/224 [==============================] - 10s 47ms/step - loss: 0.8489 - acc: 0.6823 - val_loss: 1.0247 - val_acc: 0.6303\n",
            "Epoch 199/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8548 - acc: 0.6809 - val_loss: 1.0243 - val_acc: 0.6305\n",
            "Epoch 200/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8511 - acc: 0.6854 - val_loss: 1.0245 - val_acc: 0.6303\n",
            "Epoch 201/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8598 - acc: 0.6793 - val_loss: 1.0249 - val_acc: 0.6311\n",
            "Epoch 202/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8508 - acc: 0.6817 - val_loss: 1.0246 - val_acc: 0.6297\n",
            "Epoch 203/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8560 - acc: 0.6804 - val_loss: 1.0240 - val_acc: 0.6300\n",
            "Epoch 204/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8487 - acc: 0.6850 - val_loss: 1.0252 - val_acc: 0.6308\n",
            "Epoch 205/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8523 - acc: 0.6805 - val_loss: 1.0242 - val_acc: 0.6305\n",
            "Epoch 206/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8498 - acc: 0.6822 - val_loss: 1.0244 - val_acc: 0.6297\n",
            "Epoch 207/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8511 - acc: 0.6858 - val_loss: 1.0247 - val_acc: 0.6308\n",
            "Epoch 208/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8510 - acc: 0.6838 - val_loss: 1.0246 - val_acc: 0.6297\n",
            "Epoch 209/300\n",
            "224/224 [==============================] - 15s 65ms/step - loss: 0.8471 - acc: 0.6867 - val_loss: 1.0244 - val_acc: 0.6300\n",
            "Epoch 210/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8535 - acc: 0.6831 - val_loss: 1.0246 - val_acc: 0.6294\n",
            "Epoch 211/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 0.8536 - acc: 0.6815 - val_loss: 1.0246 - val_acc: 0.6303\n",
            "Epoch 212/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8474 - acc: 0.6843 - val_loss: 1.0244 - val_acc: 0.6303\n",
            "Epoch 213/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8525 - acc: 0.6816 - val_loss: 1.0243 - val_acc: 0.6303\n",
            "Epoch 214/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8569 - acc: 0.6774 - val_loss: 1.0248 - val_acc: 0.6300\n",
            "Epoch 215/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8549 - acc: 0.6778 - val_loss: 1.0244 - val_acc: 0.6300\n",
            "Epoch 216/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8542 - acc: 0.6831 - val_loss: 1.0245 - val_acc: 0.6303\n",
            "Epoch 217/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8497 - acc: 0.6839 - val_loss: 1.0243 - val_acc: 0.6303\n",
            "Epoch 218/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8528 - acc: 0.6808 - val_loss: 1.0239 - val_acc: 0.6311\n",
            "Epoch 219/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8525 - acc: 0.6814 - val_loss: 1.0245 - val_acc: 0.6303\n",
            "Epoch 220/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8473 - acc: 0.6839 - val_loss: 1.0242 - val_acc: 0.6300\n",
            "Epoch 221/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8563 - acc: 0.6809 - val_loss: 1.0240 - val_acc: 0.6303\n",
            "Epoch 222/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8541 - acc: 0.6793 - val_loss: 1.0242 - val_acc: 0.6300\n",
            "Epoch 223/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8485 - acc: 0.6819 - val_loss: 1.0242 - val_acc: 0.6305\n",
            "Epoch 224/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8502 - acc: 0.6833 - val_loss: 1.0242 - val_acc: 0.6308\n",
            "Epoch 225/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8539 - acc: 0.6823 - val_loss: 1.0246 - val_acc: 0.6314\n",
            "Epoch 226/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8529 - acc: 0.6797 - val_loss: 1.0244 - val_acc: 0.6308\n",
            "Epoch 227/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8531 - acc: 0.6812 - val_loss: 1.0246 - val_acc: 0.6305\n",
            "Epoch 228/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8506 - acc: 0.6839 - val_loss: 1.0243 - val_acc: 0.6311\n",
            "Epoch 229/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8507 - acc: 0.6847 - val_loss: 1.0243 - val_acc: 0.6303\n",
            "Epoch 230/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8527 - acc: 0.6831 - val_loss: 1.0245 - val_acc: 0.6303\n",
            "Epoch 231/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8487 - acc: 0.6798 - val_loss: 1.0245 - val_acc: 0.6305\n",
            "Epoch 232/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8594 - acc: 0.6796 - val_loss: 1.0245 - val_acc: 0.6297\n",
            "Epoch 233/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8517 - acc: 0.6803 - val_loss: 1.0250 - val_acc: 0.6303\n",
            "Epoch 234/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8459 - acc: 0.6838 - val_loss: 1.0244 - val_acc: 0.6297\n",
            "Epoch 235/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8534 - acc: 0.6819 - val_loss: 1.0239 - val_acc: 0.6300\n",
            "Epoch 236/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8532 - acc: 0.6837 - val_loss: 1.0245 - val_acc: 0.6294\n",
            "Epoch 237/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8479 - acc: 0.6819 - val_loss: 1.0245 - val_acc: 0.6303\n",
            "Epoch 238/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8489 - acc: 0.6821 - val_loss: 1.0241 - val_acc: 0.6305\n",
            "Epoch 239/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8579 - acc: 0.6770 - val_loss: 1.0240 - val_acc: 0.6297\n",
            "Epoch 240/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8523 - acc: 0.6832 - val_loss: 1.0244 - val_acc: 0.6303\n",
            "Epoch 241/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8543 - acc: 0.6788 - val_loss: 1.0245 - val_acc: 0.6303\n",
            "Epoch 242/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8503 - acc: 0.6828 - val_loss: 1.0243 - val_acc: 0.6303\n",
            "Epoch 243/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8571 - acc: 0.6804 - val_loss: 1.0242 - val_acc: 0.6305\n",
            "Epoch 244/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8501 - acc: 0.6835 - val_loss: 1.0247 - val_acc: 0.6300\n",
            "Epoch 245/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8537 - acc: 0.6815 - val_loss: 1.0246 - val_acc: 0.6297\n",
            "Epoch 246/300\n",
            "224/224 [==============================] - 12s 51ms/step - loss: 0.8531 - acc: 0.6822 - val_loss: 1.0242 - val_acc: 0.6297\n",
            "Epoch 247/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8500 - acc: 0.6829 - val_loss: 1.0243 - val_acc: 0.6303\n",
            "Epoch 248/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8535 - acc: 0.6823 - val_loss: 1.0245 - val_acc: 0.6305\n",
            "Epoch 249/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8487 - acc: 0.6839 - val_loss: 1.0242 - val_acc: 0.6294\n",
            "Epoch 250/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8552 - acc: 0.6767 - val_loss: 1.0242 - val_acc: 0.6303\n",
            "Epoch 251/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8495 - acc: 0.6843 - val_loss: 1.0244 - val_acc: 0.6303\n",
            "Epoch 252/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8522 - acc: 0.6809 - val_loss: 1.0243 - val_acc: 0.6300\n",
            "Epoch 253/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8523 - acc: 0.6850 - val_loss: 1.0242 - val_acc: 0.6303\n",
            "Epoch 254/300\n",
            "224/224 [==============================] - 12s 51ms/step - loss: 0.8490 - acc: 0.6846 - val_loss: 1.0246 - val_acc: 0.6300\n",
            "Epoch 255/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8547 - acc: 0.6805 - val_loss: 1.0243 - val_acc: 0.6305\n",
            "Epoch 256/300\n",
            "224/224 [==============================] - 12s 51ms/step - loss: 0.8545 - acc: 0.6781 - val_loss: 1.0245 - val_acc: 0.6305\n",
            "Epoch 257/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8485 - acc: 0.6853 - val_loss: 1.0244 - val_acc: 0.6297\n",
            "Epoch 258/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8536 - acc: 0.6811 - val_loss: 1.0245 - val_acc: 0.6289\n",
            "Epoch 259/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8494 - acc: 0.6872 - val_loss: 1.0247 - val_acc: 0.6303\n",
            "Epoch 260/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8503 - acc: 0.6841 - val_loss: 1.0242 - val_acc: 0.6305\n",
            "Epoch 261/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8500 - acc: 0.6840 - val_loss: 1.0245 - val_acc: 0.6305\n",
            "Epoch 262/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8495 - acc: 0.6837 - val_loss: 1.0241 - val_acc: 0.6303\n",
            "Epoch 263/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8495 - acc: 0.6824 - val_loss: 1.0242 - val_acc: 0.6297\n",
            "Epoch 264/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8477 - acc: 0.6833 - val_loss: 1.0240 - val_acc: 0.6303\n",
            "Epoch 265/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8579 - acc: 0.6804 - val_loss: 1.0240 - val_acc: 0.6300\n",
            "Epoch 266/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8508 - acc: 0.6816 - val_loss: 1.0243 - val_acc: 0.6300\n",
            "Epoch 267/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8485 - acc: 0.6818 - val_loss: 1.0244 - val_acc: 0.6303\n",
            "Epoch 268/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8481 - acc: 0.6836 - val_loss: 1.0244 - val_acc: 0.6300\n",
            "Epoch 269/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8572 - acc: 0.6780 - val_loss: 1.0249 - val_acc: 0.6303\n",
            "Epoch 270/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8476 - acc: 0.6852 - val_loss: 1.0241 - val_acc: 0.6303\n",
            "Epoch 271/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8551 - acc: 0.6812 - val_loss: 1.0242 - val_acc: 0.6303\n",
            "Epoch 272/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8550 - acc: 0.6799 - val_loss: 1.0245 - val_acc: 0.6300\n",
            "Epoch 273/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8582 - acc: 0.6802 - val_loss: 1.0245 - val_acc: 0.6303\n",
            "Epoch 274/300\n",
            "224/224 [==============================] - 12s 52ms/step - loss: 0.8533 - acc: 0.6809 - val_loss: 1.0242 - val_acc: 0.6305\n",
            "Epoch 275/300\n",
            "224/224 [==============================] - 11s 51ms/step - loss: 0.8505 - acc: 0.6807 - val_loss: 1.0242 - val_acc: 0.6300\n",
            "Epoch 276/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8507 - acc: 0.6854 - val_loss: 1.0244 - val_acc: 0.6303\n",
            "Epoch 277/300\n",
            "224/224 [==============================] - 11s 48ms/step - loss: 0.8496 - acc: 0.6841 - val_loss: 1.0247 - val_acc: 0.6303\n",
            "Epoch 278/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8489 - acc: 0.6816 - val_loss: 1.0241 - val_acc: 0.6305\n",
            "Epoch 279/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8545 - acc: 0.6807 - val_loss: 1.0245 - val_acc: 0.6305\n",
            "Epoch 280/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8527 - acc: 0.6812 - val_loss: 1.0245 - val_acc: 0.6303\n",
            "Epoch 281/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8516 - acc: 0.6837 - val_loss: 1.0245 - val_acc: 0.6311\n",
            "Epoch 282/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8540 - acc: 0.6812 - val_loss: 1.0239 - val_acc: 0.6305\n",
            "Epoch 283/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8519 - acc: 0.6808 - val_loss: 1.0243 - val_acc: 0.6300\n",
            "Epoch 284/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8551 - acc: 0.6811 - val_loss: 1.0244 - val_acc: 0.6303\n",
            "Epoch 285/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8586 - acc: 0.6794 - val_loss: 1.0245 - val_acc: 0.6300\n",
            "Epoch 286/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8595 - acc: 0.6811 - val_loss: 1.0246 - val_acc: 0.6300\n",
            "Epoch 287/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8552 - acc: 0.6813 - val_loss: 1.0244 - val_acc: 0.6303\n",
            "Epoch 288/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8592 - acc: 0.6787 - val_loss: 1.0244 - val_acc: 0.6308\n",
            "Epoch 289/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8458 - acc: 0.6838 - val_loss: 1.0246 - val_acc: 0.6303\n",
            "Epoch 290/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8475 - acc: 0.6857 - val_loss: 1.0243 - val_acc: 0.6311\n",
            "Epoch 291/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8546 - acc: 0.6786 - val_loss: 1.0247 - val_acc: 0.6300\n",
            "Epoch 292/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8506 - acc: 0.6833 - val_loss: 1.0244 - val_acc: 0.6311\n",
            "Epoch 293/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8522 - acc: 0.6828 - val_loss: 1.0243 - val_acc: 0.6305\n",
            "Epoch 294/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8506 - acc: 0.6822 - val_loss: 1.0243 - val_acc: 0.6311\n",
            "Epoch 295/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8577 - acc: 0.6791 - val_loss: 1.0244 - val_acc: 0.6300\n",
            "Epoch 296/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8522 - acc: 0.6823 - val_loss: 1.0243 - val_acc: 0.6303\n",
            "Epoch 297/300\n",
            "224/224 [==============================] - 11s 50ms/step - loss: 0.8505 - acc: 0.6844 - val_loss: 1.0241 - val_acc: 0.6303\n",
            "Epoch 298/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8484 - acc: 0.6849 - val_loss: 1.0242 - val_acc: 0.6308\n",
            "Epoch 299/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8595 - acc: 0.6805 - val_loss: 1.0245 - val_acc: 0.6311\n",
            "Epoch 300/300\n",
            "224/224 [==============================] - 11s 49ms/step - loss: 0.8574 - acc: 0.6809 - val_loss: 1.0243 - val_acc: 0.6308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-FSBxRPXHSZE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b2c858c5-faf3-4463-e325-4a441744c6a1"
      },
      "source": [
        "print('\\n# Evaluate on dev data')\n",
        "results_dev = model.evaluate(X_dev, Y_dev , batch_size=128)\n",
        "print('dev loss, dev acc:', results_dev)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on dev data\n",
            "3589/3589 [==============================] - 0s 72us/step\n",
            "dev loss, dev acc: [1.0242647132889477, 0.6308163833978548]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b9-RdV1zHs_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1cfd9d8d-f184-4225-f131-65c761224aed"
      },
      "source": [
        "test_dataset_dir = '/content/drive/My Drive/cs230 project/collab/fer2013/test.csv'\n",
        "X_test, Y_test      = get_data(test_dataset_dir)\n",
        "\n",
        "print('\\n# Evaluate on test data')\n",
        "results_test = model.evaluate(X_test, Y_test , batch_size=128)\n",
        "print('test loss, test acc:', results_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "3589/3589 [==============================] - 0s 72us/step\n",
            "test loss, test acc: [0.9734118991856058, 0.6475341320785183]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr6u8rO4zHtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pQfysJQzHtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_str = '-SGD_LR_%.5f' % SGD_LEARNING_RATE\n",
        "epoch_str = '-EPOCHS_' + str(EPOCHS)\n",
        "bs_str = '-BS_' + str(BS)\n",
        "dropout_str = '-DROPOUT_' + str(DROPOUT_RATE)\n",
        "test_acc = 'test_acc_%.3f' % results_test[1]\n",
        "model.save('/content/drive/My Drive/cs230 project/collab/fer2013/models/soa' + lr_str + epoch_str + bs_str + dropout_str + test_acc + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMAJ9smqKdD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# configure image data augmentation\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "# make a prediction using test-time augmentation\n",
        "def tta_prediction(datagen, model, image, n_examples):\n",
        "\t# convert image into dataset\n",
        "\tsamples = np.expand_dims(image, 0)\n",
        "\t# prepare iterator\n",
        "\tit = datagen.flow(samples, batch_size=n_examples)\n",
        "\t# make predictions for each augmented image\n",
        "\tyhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
        "\t# sum across predictions\n",
        "\tsummed = np.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\treturn np.argmax(summed)\n",
        " \n",
        " # evaluate a model on a dataset using test-time augmentation\n",
        "def tta_evaluate_model(model, testX, testY):\n",
        "\t# configure image data augmentation\n",
        "\tdatagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\t# define the number of augmented images to generate per test set image\n",
        "\tn_examples_per_image = 7\n",
        "\tyhats = list()\n",
        "\tfor i in range(len(testX)):\n",
        "\t\t# make augmented prediction\n",
        "\t\tyhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\n",
        "\t\t# store for evaluation\n",
        "\t\tyhats.append(yhat)\n",
        "\t# calculate accuracy\n",
        "\ttestY_labels = np.argmax(testY, axis=1)\n",
        "\tacc = accuracy_score(testY_labels, yhats)\n",
        "\treturn acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfTihcArMdUk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9d50b169-afcc-439f-d9a1-59421306ae9b"
      },
      "source": [
        "print('\\n# Evaluate on test data')\n",
        "TTA_results_test = tta_evaluate_model(model, X_test, Y_test)\n",
        "print('test loss, test acc:', results_test)\n",
        "print('TTA test acc:', TTA_results_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V57Y-EomzHtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}