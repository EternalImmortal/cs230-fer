{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fer2013.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amilkh/cs230-fer/blob/75-soa/fer2013-fit_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwdg7Sv3XBaP",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2nz38mJZXN_P",
        "outputId": "040508e2-2494-4150-fc8c-be1b42d2bce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI204HIYEdLE",
        "colab_type": "code",
        "outputId": "bd85d3ad-578b-45d8-9aa5-c2a07b85fe38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hAjh6yOLYPZm",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Function that reads the data from the csv file, increases the size of the images and returns the images and their labels\n",
        "def get_datagen(dataset):\n",
        "    # Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely\n",
        "    # rescale:          Rescaling factor (defaults to None). Multiply the data by the value provided (before applying any other transformation)\n",
        "    # rotation_range:   Int. Degree range for random rotations\n",
        "    # shear_range:      Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
        "    # zoom_range:       Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]\n",
        "    # fill_mode :       Points outside the boundaries of the input are filled according to the given mode: {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}\n",
        "    # horizontal_flip:  Boolean. Randomly flip inputs horizontally\n",
        "    datagen = ImageDataGenerator(\n",
        "                        rescale=1./255,\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)\n",
        "\n",
        "    return datagen.flow_from_directory(\n",
        "            dataset,\n",
        "            target_size=(48, 48),\n",
        "            color_mode='grayscale',\n",
        "            shuffle = False,\n",
        "            class_mode='categorical',\n",
        "            batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYmxfy1zSrBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d9e61fbb-94e6-441a-d415-e41d71e45eb9"
      },
      "source": [
        "%%bash\n",
        "root='/content/drive/My Drive/cs230 project/dataset/fer2013/data_fer_images/test/'\n",
        "IFS=$(echo -en \"\\n\\b\")\n",
        "(for dir in $(ls -1 \"$root\")\n",
        "    do printf \"$dir: \" && ls -i \"$root$dir\" | wc -l\n",
        " done)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "angry: 958\n",
            "disgusted: 111\n",
            "fearful: 1024\n",
            "happy: 1774\n",
            "neutral: 1233\n",
            "sad: 1247\n",
            "surprised: 831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm5tqGr-zHsm",
        "colab_type": "code",
        "outputId": "2217621e-b823-4fec-c051-da7219128108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "training_dataset_dir = '/content/drive/My Drive/cs230 project/dataset/fer2013/test'\n",
        "dev_dataset_dir = '/content/drive/My Drive/cs230 project/dataset/webcam'\n",
        "#TODO split into dev/test folders\n",
        "\n",
        "X_train_gen  = get_datagen(training_dataset_dir)\n",
        "X_dev_gen    = get_datagen(dev_dataset_dir)\n",
        "\n",
        "# Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batcfillhes indefinitely, in an infinite loop\n",
        "# x:            Data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, \n",
        "#               it should have value 3\n",
        "# y:            Labels\n",
        "# batch_size:   Int (default: 32)\n",
        "#train_generator = train_datagen.flow(X_train, Y_train,  batch_size  = BS)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7178 images belonging to 7 classes.\n",
            "Found 130 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPO33wZKzHsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BS = 128\n",
        "DROPOUT_RATE = 0.4\n",
        "SGD_LEARNING_RATE = 0.01\n",
        "SGD_DECAY = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG4t-6RO-g2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement below paper CPCPCPFF depth 5, 2.4m params\n",
        "# http://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Kim_Fusing_Aligned_and_CVPR_2016_paper.pdf\n",
        "# Reference: https://arxiv.org/pdf/1612.02903.pdf\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5, 5), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
        "#model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(32, (4, 4), activation='relu',padding='same',name=\"conv2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
        "#model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu',padding='same',name=\"conv3\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu',name='fc1'))\n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
        "\n",
        "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
        "sgd = SGD(lr=SGD_LEARNING_RATE,momentum=0.9, decay=SGD_DECAY, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "#rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)\n",
        "rlrop = ReduceLROnPlateau(monitor='val_acc',mode='max',factor=0.5, patience=10, min_lr=0.00001, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ3hGX2HzHsw",
        "colab_type": "code",
        "outputId": "864a3559-730b-4074-9154-2e4246b3abb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "history = model.fit_generator(\n",
        "    generator = X_train_gen,\n",
        "    validation_data = X_dev_gen, \n",
        "    steps_per_epoch=len(X_train_gen.filenames) // BS,\n",
        "    shuffle=True,\n",
        "    callbacks=[rlrop],\n",
        "    epochs=EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "39/56 [===================>..........] - ETA: 3:11 - loss: 7.9203 - acc: 0.1322"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr6u8rO4zHtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pQfysJQzHtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model to disk\n",
        "lr_str = '-SGD_LR_%.5f' % SGD_LEARNING_RATE\n",
        "epoch_str = '-EPOCHS_' + str(EPOCHS)\n",
        "bs_str = '-BS_' + str(BS)\n",
        "dropout_str = '-DROPOUT_' + str(DROPOUT_RATE)\n",
        "test_acc = 'test_acc_%.3f' % results_test[1]\n",
        "model.save('/content/drive/My Drive/cs230 project/models/soa' + lr_str + epoch_str + bs_str + dropout_str + test_acc + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMAJ9smqKdD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# configure image data augmentation\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "# make a prediction using test-time augmentation\n",
        "def tta_prediction(datagen, model, image, n_examples):\n",
        "\t# convert image into dataset\n",
        "\tsamples = np.expand_dims(image, 0)\n",
        "\t# prepare iterator\n",
        "\tit = datagen.flow(samples, batch_size=n_examples)\n",
        "\t# make predictions for each augmented image\n",
        "\tyhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
        "\t# sum across predictions\n",
        "\tsummed = np.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\treturn np.argmax(summed)\n",
        " \n",
        " # evaluate a model on a dataset using test-time augmentation\n",
        "def tta_evaluate_model(model, testX, testY):\n",
        "\t# configure image data augmentation\n",
        "\tdatagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\t# define the number of augmented images to generate per test set image\n",
        "\tn_examples_per_image = 7\n",
        "\tyhats = list()\n",
        "\tfor i in range(len(testX)):\n",
        "\t\t# make augmented prediction\n",
        "\t\tyhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\n",
        "\t\t# store for evaluation\n",
        "\t\tyhats.append(yhat)\n",
        "\t# calculate accuracy\n",
        "\ttestY_labels = np.argmax(testY, axis=1)\n",
        "\tacc = accuracy_score(testY_labels, yhats)\n",
        "\treturn acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfTihcArMdUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\n# Evaluate on test data')\n",
        "TTA_results_test = tta_evaluate_model(model, X_test, Y_test)\n",
        "print('test loss, test acc:', results_test)\n",
        "print('TTA test acc:', TTA_results_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}